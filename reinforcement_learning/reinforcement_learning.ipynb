{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCEMENT LEARNING (RL) – POLICY GRADIENTS\n",
    "\n",
    "- https://theneuralperspective.com/2016/11/25/reinforcement-learning-rl-policy-gradients-i/\n",
    "- https://theneuralperspective.com/2016/11/26/1656/\n",
    "- https://github.com/ashutoshkrjha/Cartpole-OpenAI-Tensorflow/blob/master/cartpole.py\n",
    "- https://gist.github.com/shanest/535acf4c62ee2a71da498281c2dfc4f4\n",
    "\n",
    "\n",
    "- https://github.com/dennybritz/reinforcement-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OBJECTIVE\n",
    "\n",
    "The three main components in our model include the state, action and reward. The state can be thought of the environment which generates an action which leads to a reward. Actions can also alter the state and often the reward may be delayed and not always an immediate response to a given action.\n",
    "\n",
    "```    \n",
    "               -----------------\n",
    "              |                 |\n",
    "             \\ /                |\n",
    "              -                 |\n",
    "            State-----------> Action-----------> Reward\n",
    "              |                                    -\n",
    "              |                                   / \\\n",
    "              |                                    |\n",
    "              --------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s  ---> state   \n",
    "a  ---> action   \n",
    "a* ---> correct action  \n",
    "r  ---> reward   \n",
    "$\\pi$  ---> policy  \n",
    "$\\theta$  --->  policy weights   \n",
    "R ---> total reward   \n",
    "$\\hat A $  ---> Advantage Est   \n",
    "$\\gamma$  ---> discount factor   \n",
    "\n",
    "$max_\\theta \\sum_{n=1}^{N}\\ \\log P(y_n|x_n;\\theta)$    \n",
    "= $max_\\theta \\sum_{n=1}^{N}\\ \\log P(a^*_n|s_n;\\theta)$    \n",
    "= $min_\\theta \\big[- \\sum_{n=1}^{N}\\ \\log P(a^*_n|s_n;\\theta)\\big]$ \n",
    "\n",
    "Since we don’t have the correction actions to take, the best we can do it try some actions that may turn our good/bad and then eventually train our policy weights (theta) so that we increase the chances of the good actions. One common approach is to collect a series of states, actions and the corresponding rewards (s0, a0, r0, s1, a1, r1, … ) and from this we can calculate R, the total reward – sum of all the rewards r. This will give us the policy gradient:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\theta} = \\frac{\\partial \\sum \\log \\pi (a|s;\\theta)}{\\partial \\theta} R\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take a closer look at why this policy gradient is expressed as it is later in the section where we draw parallels with supervised learning.\n",
    "\n",
    "When calculating the total reward R for an episode (series of events), we will have good and bad actions. But, according to our policy gradient, we will be updating the weights to favor ALL the actions in a given episode if the reward is positive and the magnitude of the update depends on the magnitude of the gradient. When we repeat this with enough episodes, our policy gradient becomes quite precise in modeling what actions to take given a state in order to maximize the reward.\n",
    "\n",
    "There are several additions we can make to our policy gradient, such as adding on an advantage estimator to determine which specific actions were good/bad instead of just using the total reward to judge an episode as good/bad.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how does this supervised learning technique relate to reinforcement learning policy gradients? If you inspect the loss functions, you will see that they are exactly the same in principle. The negative log likelihood loss function above is the simplification of the multi-class cross entropy loss function below.\n",
    "\n",
    "$$\n",
    "J(\\theta) = - \\sum_i y_i ln(\\hat y_i)\n",
    "$$\n",
    "\n",
    "Eg:   \n",
    "Computed($\\hat y$): [0.3,0.3,0.4]  \n",
    "Targets ($y$) : [0,0,1]   \n",
    "$$\n",
    "J(\\theta) = -  [0 * ln(0.3) + 0 * ln(0.3) + 1 * ln(0.4)] = - ln(0.4)\n",
    "$$\n",
    "\n",
    "With the multinomial cross entropy, you can see that we only keep the loss contribution from the correct class. Usually, with neural nets, this will be case if our ouputs are sparse (just 1 true class). Therefore, we can rewrite our loss into just a $\\sum(-log(\\hat y))$ where y_hat will just be the probability of the correct class. We just replace $y_i$ (true y) with 1 and for the probabilities for the other classes, doesn’t matter because their $y_i$ is 0. This is referred to as negative log likelihood. But for drawing the parallel between supervised learning and RL, let’s keep it in the explicit cross-entropy form.\n",
    "\n",
    "Supervised: $J(\\theta) = - \\sum y_i \\log (\\hat y_i)$    \n",
    "Reninforcement: $J(\\theta) = - \\sum r \\log \\pi (a|s;\\theta)$\n",
    "\n",
    "\n",
    "In supervised learning, we have a prediction ($\\hat y$) and a true label ($y_i$). In a typical case, only one label will be 1 (true) in $y_i$, so therefore only the log of the true class’s prediction will be taken into account. But as we saw above, the gradient will take into account the predicted probability for all the classes. In RL, we have our action (a) based on our policy ($\\pi$) which we take the log of. We multiply the action from the policy with our reward for that action. \n",
    "\n",
    "Note: The action is a number from the outputs (ex. chosen action is 2, so we take the 0.9 from  [0.2, 0.3, 0.9, 0.1] to put into the log . The reward can be any magnitude and direction but like our supervised case, it will help determine the loss and properly adjust the weights by influencing the gradient. If the reward is positive, the weights will be altered via the gradient in order to favor the action that was made in that state. If the reward is negative, the gradient will be unfavored to make that action with that particular state. DO NOT draw parallels by saying the reward is like $y_i$ because, as you can see, that is not the case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NUANCES\n",
    "\n",
    "You may notice that we do an additional operation to our rewards before feeding it in for training. We do what’s known as discounting the reward. The idea is that each reward will be weighted by all the rewards that follow it since the action responsible for the current reward will determine the rewards for the subsequent events. We weight each reward by the discount factor gamma^(time since reward). So each reward will be recalculated by the following expression:\n",
    "\n",
    "$$\n",
    "r_t = \\sum_{k=0}^{\\infty} \\gamma ^ k r_{t+1}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gym.openai.com/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-05 10:34:52,539] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward is: 19.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import time\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "total_reward = 0\n",
    "for _ in range(10000):\n",
    "    env.render()\n",
    "    time.sleep(0.1)\n",
    "    observation, reward, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    total_reward += reward\n",
    "    if done: break\n",
    "env.close()\n",
    "\n",
    "print('Total Reward is:', total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 0 0 0 "
     ]
    }
   ],
   "source": [
    "#possible actions available\n",
    "for i in range(20):\n",
    "    print(env.action_space.sample(), end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Box(4,)\n",
      "[  4.80000000e+00   3.40282347e+38   4.18879020e-01   3.40282347e+38]\n",
      "[ -4.80000000e+00  -3.40282347e+38  -4.18879020e-01  -3.40282347e+38]\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "#> Discrete(2) i.e valid actions are either 0 or 1. \n",
    "print(env.observation_space)\n",
    "#> Box(4,)\n",
    "print(env.observation_space.high)\n",
    "#> array([ 2.4       ,         inf,  0.20943951,         inf])\n",
    "print(env.observation_space.low)\n",
    "#> array([-2.4       ,        -inf, -0.20943951,        -inf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from gym import envs\n",
    "# avaiable_envs = envs.registry.all()\n",
    "# [print(each) for each in list(avaiable_envs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-05 10:35:20,341] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_initial:  [ 0.0171071  -0.02363623  0.04319796 -0.01053885]\n",
      "Episode finished after 14 timesteps with score 14.0\n",
      "Episode finished after 28 timesteps with score 28.0\n",
      "Episode finished after 16 timesteps with score 16.0\n",
      "Episode finished after 13 timesteps with score 13.0\n",
      "Episode finished after 13 timesteps with score 13.0\n",
      "Episode finished after 28 timesteps with score 28.0\n",
      "Episode finished after 13 timesteps with score 13.0\n",
      "Episode finished after 21 timesteps with score 21.0\n",
      "Episode finished after 24 timesteps with score 24.0\n",
      "Episode finished after 15 timesteps with score 15.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "input_initial = env.reset()\n",
    "\n",
    "print('input_initial: ', input_initial)\n",
    "for i_episode in range(10):\n",
    "    total_reward = 0\n",
    "    observation = env.reset()\n",
    "    for t in range(1000):\n",
    "        env.render()\n",
    "        time.sleep(0.01)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            \n",
    "#             print(observation, reward, done, info )\n",
    "            print(\"Episode finished after {} timesteps with score {}\".format(t+1, total_reward))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mageswarand/anaconda3/envs/tensorflow1.0/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import json\n",
    "import os, inspect\n",
    "import math\n",
    "sys.path.append(\"../\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "env.reset()\n",
    "\n",
    "# Hyperparameters\n",
    "H_SIZE = 10  # Number of hidden layer neurons\n",
    "batch_size = 5  # Update Params after every 5 episodes\n",
    "ETA = 1e-2  # Learning Rate\n",
    "GAMMA = 0.99  # Discount factor\n",
    "\n",
    "INPUT_DIM = 4  # Input dimensions\n",
    "\n",
    "# Initializing\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-09 21:39:58,084] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "# Network to define moving left or right\n",
    "input = tf.placeholder(tf.float32, [None, INPUT_DIM], name=\"input_x\")\n",
    "W1 = tf.get_variable(\"W1\", shape=[INPUT_DIM, H_SIZE],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "layer1 = tf.nn.relu(tf.matmul(input, W1))\n",
    "W2 = tf.get_variable(\"W2\", shape=[H_SIZE, 1],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "score = tf.matmul(layer1, W2)\n",
    "probability = tf.nn.sigmoid(score)\n",
    "\n",
    "# From here we define the parts of the network needed for learning a good policy.\n",
    "tvars = tf.trainable_variables()\n",
    "input_y = tf.placeholder(tf.float32, [None, 1], name=\"input_y\")\n",
    "advantages = tf.placeholder(tf.float32, name=\"reward_signal\")\n",
    "\n",
    "# The loss function. This sends the weights in the direction of making actions\n",
    "# that gave good advantage (reward over time) more likely, and actions that didn't less likely.\n",
    "loglik = tf.log(input_y * (input_y - probability) + (1 - input_y) * (input_y + probability))\n",
    "loss = -tf.reduce_mean(loglik * advantages)\n",
    "newGrads = tf.gradients(loss, tvars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adam = tf.train.AdamOptimizer(learning_rate=ETA)  # Adam optimizer\n",
    "W1Grad = tf.placeholder(tf.float32, name=\"batch_grad1\")  # Placeholders for final gradients once update happens\n",
    "W2Grad = tf.placeholder(tf.float32, name=\"batch_grad2\")\n",
    "batchGrad = [W1Grad, W2Grad]\n",
    "updateGrads = adam.apply_gradients(zip(batchGrad, tvars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        running_add = running_add * GAMMA + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "\n",
    "xs, hs, drs, ys = [], [], [], []  # Arrays to store parameters till an update happens\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 1\n",
    "total_episodes = 10000\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "with tf.Session() as sess:\n",
    "    rendering = False\n",
    "    sess.run(init)\n",
    "    input_initial = env.reset()  # Initial state of the environment\n",
    "\n",
    "    # Array to store gradients for each min-batch step\n",
    "    gradBuffer = sess.run(tvars)\n",
    "    for ix, grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "\n",
    "    while episode_number <= total_episodes:\n",
    "\n",
    "        if reward_sum / batch_size > 100 or rendering == True:  # Render environment only after avg reward reaches 100\n",
    "            env.render()\n",
    "            rendering = True\n",
    "\n",
    "        # Format the state for placeholder\n",
    "        x = np.reshape(input_initial, [1, INPUT_DIM])\n",
    "\n",
    "        # Run policy network\n",
    "        tfprob = sess.run(probability, feed_dict={input: x})\n",
    "        action = 1 if np.random.uniform() < tfprob else 0\n",
    "\n",
    "        xs.append(x)  # Store x\n",
    "        y = 1 if action == 0 else 0\n",
    "        ys.append(y)\n",
    "\n",
    "        # take action for the state\n",
    "        input_initial, reward, done, info = env.step(action)\n",
    "        reward_sum += reward\n",
    "\n",
    "        drs.append(reward)  # store reward after action is taken\n",
    "\n",
    "        if done:\n",
    "            episode_number += 1\n",
    "            # Stack the memory arrays to feed in session\n",
    "            epx = np.vstack(xs)\n",
    "            epy = np.vstack(ys)\n",
    "            epr = np.vstack(drs)\n",
    "\n",
    "            xs, hs, drs, ys = [], [], [], []  # Reset Arrays\n",
    "\n",
    "            # Compute the discounted reward\n",
    "            discounted_epr = discount_rewards(epr)\n",
    "\n",
    "            discounted_epr -= np.mean(discounted_epr)\n",
    "            discounted_epr /= np.std(discounted_epr)\n",
    "\n",
    "            # Get and save the gradient\n",
    "            tGrad = sess.run(newGrads, feed_dict={input: epx, input_y: epy, advantages: discounted_epr})\n",
    "            for ix, grad in enumerate(tGrad):\n",
    "                gradBuffer[ix] += grad\n",
    "\n",
    "            # Update Params after Min-Batch number of episodes\n",
    "            if episode_number % batch_size == 0:\n",
    "                sess.run(updateGrads, feed_dict={W1Grad: gradBuffer[0], W2Grad: gradBuffer[1]})\n",
    "                for ix, grad in enumerate(gradBuffer):\n",
    "                    gradBuffer[ix] = grad * 0\n",
    "\n",
    "                # Print details of the present model\n",
    "                running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "                print\n",
    "                'Average reward for episode %f.  Total average reward %f.' % (\n",
    "                reward_sum / batch_size, running_reward / batch_size)\n",
    "\n",
    "                if reward_sum / batch_size > 200:\n",
    "                    print\n",
    "                    \"Task solved in\", episode_number, 'episodes'\n",
    "                    break\n",
    "\n",
    "                reward_sum = 0\n",
    "\n",
    "            input_initial = env.reset()\n",
    "\n",
    "print(episode_number, 'Episodes completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/ashutoshkrjha/Cartpole-OpenAI-Tensorflow\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from overrides import overrides\n",
    "\n",
    "\n",
    "#TensorFlow\n",
    "from dhira.tf.models.internal.base_tf_model import BaseTFModel\n",
    "\n",
    "class PolicyGradient(BaseTFModel):\n",
    "    def __init__(self,\n",
    "                 name='PlocyGradient',\n",
    "                 run_id=0,\n",
    "                 save_dir=None,\n",
    "                 log_dir=None):\n",
    "        super(self.__class__, self).__init__(name=name,\n",
    "                 run_id=run_id,\n",
    "                 save_dir=save_dir,\n",
    "                 log_dir=log_dir)\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.H_SIZE = 10  # Number of hidden layer neurons\n",
    "        self.ETA = 1e-2  # Learning Rate\n",
    "        self.GAMMA = 0.99  # Discount factor\n",
    "\n",
    "        self.INPUT_DIM = 4  # Input dimensions\n",
    "\n",
    "    def _create_placeholders(self):\n",
    "        # Network to define moving left or right\n",
    "        self.observations = tf.placeholder(tf.float32, [None, self.INPUT_DIM], name=\"observations\")\n",
    "        self.actions = tf.placeholder(tf.float32, [None, 1], name=\"action\")\n",
    "        # self.reward = tf.placeholder(tf.float32, name=\"reward_signal\")\n",
    "        self.rewards = tf.placeholder(tf.float32, [None, 1], name=\"reward_signal\")\n",
    "\n",
    "\n",
    "    @overrides\n",
    "    def _setup_graph_def(self):\n",
    "        layer = tf.layers.dense(inputs=self.observations,\n",
    "                                units=self.H_SIZE,\n",
    "                                activation=tf.nn.relu,\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.pred_actions = tf.layers.dense(inputs=layer,\n",
    "                                           units=1,\n",
    "                                           activation=tf.nn.sigmoid,\n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "\n",
    "        # The loss function. This sends the weights in the direction of making actions\n",
    "        # that gave good advantage (reward over time) more likely, and actions that didn't less likely.\n",
    "        # log(y * (y - y^) + (1 - y) * (y + y^))\n",
    "        self._loglik = tf.log(self.actions * (self.actions - self.pred_actions) +\n",
    "                        (1 - self.actions) * (self.actions + self.pred_actions))\n",
    "        self._loss = -tf.reduce_mean(self._loglik * self.rewards)\n",
    "\n",
    "        #  mean(log(y * log(y^) + (1 - y) * log(1 - y^))) * rewards\n",
    "        # self._loss = - tf.reduce_mean((self.actions * tf.log(self.pred_action) +\n",
    "        #                               (1 - self.actions) * (tf.log(1 - self.pred_action))) *\n",
    "        #                               self.rewards,0)\n",
    "\n",
    "\n",
    "        self._optimizer = tf.train.AdamOptimizer(learning_rate=self.ETA).minimize(self._loss)#, global_step=self.global_step)  # Adam optimizer\n",
    "\n",
    "    @overrides\n",
    "    def _get_eval_metric(self):\n",
    "        return self._loss\n",
    "\n",
    "    @overrides\n",
    "    def _get_prediction(self):\n",
    "        return self.pred_actions\n",
    "\n",
    "    @overrides\n",
    "    def _get_optimizer(self):\n",
    "        return self._optimizer\n",
    "\n",
    "    @overrides\n",
    "    def _get_loss(self):\n",
    "        return self._loss\n",
    "\n",
    "    # def discount_rewards(self, r, GAMMA=0.99):\n",
    "    #     \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    #     discounted_r = np.zeros_like(r)\n",
    "    #     running_add = 0\n",
    "    #     for t in reversed(range(0, r.size)):\n",
    "    #         running_add = running_add * GAMMA + r[t]\n",
    "    #         discounted_r[t] = running_add\n",
    "    #     return discounted_r\n",
    "\n",
    "    def discount_rewards(self, rewards, gamma):\n",
    "        \"\"\"\n",
    "        Return discounted rewards weighed by gamma.\n",
    "        Each reward will be replaced with a weight reward that\n",
    "        involves itself and all the other rewards occuring after it.\n",
    "        The later the reward after it happens, the less effect it\n",
    "        has on the current rewards's discounted reward since gamma&amp;lt;1.\n",
    "\n",
    "        [r0, r1, r2, ..., r_N] will look someting like:\n",
    "        [(r0 + r1*gamma^1 + ... r_N*gamma^N), (r1 + r2*gamma^1 + ...), ...]\n",
    "        \"\"\"\n",
    "        return np.array([sum([gamma ** t * r for t, r in enumerate(rewards[i:])])\n",
    "                         for i in range(len(rewards))])\n",
    "\n",
    "    @overrides\n",
    "    def _get_train_feed_dict(self, batch, is_done):\n",
    "        inputs, lables = batch\n",
    "        observation, reward = inputs\n",
    "        if is_done is True:\n",
    "            # Compute the discounted reward\n",
    "            reward = np.vstack(\n",
    "                self.discount_rewards(reward, self.GAMMA))\n",
    "\n",
    "            discounted_epr = self.discount_rewards(reward, self.GAMMA)\n",
    "\n",
    "            discounted_epr -= np.mean(discounted_epr)\n",
    "            discounted_epr /= np.std(discounted_epr)\n",
    "\n",
    "            return {self.actions:lables[0], self.observations: observation[0], self.rewards:discounted_epr}\n",
    "\n",
    "        else:\n",
    "            return {self.observations: observation[0]}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
