{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "https://github.com/JohnSnowLabs/spark-nlp\n",
    "\n",
    "http://nlp.johnsnowlabs.com/quickstart.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post we explore the light weight third party library called John Snow Spark NLP. This post is for absolute beginner who are trying to use the JSNLP library. We asusume that you have worked with functions like stop word remover from Spark library. As compared to StanfordNLP this is very lightweight but doesnt provide indepth functionality like StanfordNLP. Yet it is good choice for operations like pos tagging and stemming. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So what can John Snow do ? \n",
    "\n",
    "It can perform following functions. \n",
    "\n",
    "    Document Assembler\n",
    "    Regex Tokenizer\n",
    "    Normalizer\n",
    "    Stemmer\n",
    "    Lemmatizer\n",
    "    Regex Matcher\n",
    "    Entity Extractor\n",
    "    Date Matcher\n",
    "    Sentence Detector\n",
    "    POSTagger\n",
    "    Sentiment Detector\n",
    "    NERTagger\n",
    "    Spell Checker\n",
    "    Vivekn Sentiment Detector\n",
    "    Finisher\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operaions listed above can be used as normal function calls like transformers as well as in Pipeline for training a machine learning algorithm.\n",
    "The difference in spark inbuilt transformers like stop word remover and JSN functions is that, JSN provides output with annotation.\n",
    "\n",
    "An annotation is the basic form of the result of a Spark-NLP operation. It's structure is made of:\n",
    "\n",
    "    annotatorType: which annotator generated this annotation\n",
    "    begin: the begin of the matched content relative to raw-text\n",
    "    end: the end of the matched content relative to raw-text\n",
    "    metadata: content of matched result and additional information\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "In order to use this library in your project either use the Jar of the project or use the sbt co-ordinates for the JSL\n",
    "\n",
    "The code below provides example of using the jar in apache toree for spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%AddJar http://dl.bintray.com/spark-packages/maven/JohnSnowLabs/spark-nlp/1.2.2/spark-nlp-1.2.2.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to import the classes. We will load the tranformers and demonstrate the use of porter stemmer inbuilt into the syste."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "import com.johnsnowlabs.nlp._\n",
    "import com.johnsnowlabs.nlp.annotators._\n",
    "import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetectorModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the file from the correct location. Here we are taking sample of the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val mobilePatsDsInitial = spark.read.json(\"hdfs://XXXX.XXXX.XXXXX/ABC.json\").\n",
    "sample(false, 0.001).\n",
    "orderBy(rand()).\n",
    "persist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create pipeline for processing the tokens. We wish to perform tokenization and stop word removal (SWR) first, reason being the \n",
    "if stop word removal is done after stemming it might change the structure of the words and SWR will miss some of the words\n",
    "\n",
    "So in PipeLine1->Tokenization -> Stop Word Removal-> (Convert back to Sentence)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "val pipelineOne = new Pipeline().setStages(Array(new org.apache.spark.ml.feature.RegexTokenizer().\n",
    "                                                    setMinTokenLength(3).\n",
    "                                                    setInputCol(\"description\").\n",
    "                                                    setOutputCol(\"descriptionTokens\"),\n",
    "                                              new StopWordsRemover().\n",
    "                                                  setInputCol(\"descriptionTokens\").\n",
    "                                                  setOutputCol(\"swrTokens\")\n",
    "                                             ))\n",
    "\n",
    "val swrDf = pipelineOne.fit(mobilePatsDsInitial).transform(mobilePatsDsInitial).drop(\"descriptionTokens\")\n",
    "\n",
    "def makeStringFn(row: Any) = row.asInstanceOf[WrappedArray[String]].mkString(\" \")\n",
    "\n",
    "val makeStringUdf = udf(makeStringFn(_: Any))\n",
    "\n",
    "val mobilePatsDs = swrDf.withColumn(\"Description\",makeStringUdf(swrDf(\"swrTokens\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "val documentAssembler = new DocumentAssembler().\n",
    "    setInputCol(\"description\").\n",
    "    setOutputCol(\"document\")\n",
    "\n",
    "val sentenceDetector = new SentenceDetectorModel().\n",
    "    setInputCols(Array(\"document\")).\n",
    "    setOutputCol(\"sentence\")\n",
    "\n",
    "val regexTokenizer = new RegexTokenizer().\n",
    "    setPattern(\"\\\\w+|\\\\$[\\\\d\\\\.]+|\\\\S+\").\n",
    "    setInputCols(Array(\"sentence\")).\n",
    "    setOutputCol(\"token\")\n",
    "\n",
    "val normalizer = new Normalizer().\n",
    "    setInputCols(Array(\"token\")).\n",
    "    setOutputCol(\"normalized\")\n",
    "\n",
    "val stemmer = new Stemmer().\n",
    "    setInputCols(Array(\"normalized\")).\n",
    "    setOutputCol(\"stem\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documentAssembler collects the column from the dataframe as text and annotates it into a document where the document has schema same\n",
    "as anotation described earlier. This step is followed by sentence detector which converts the full sentences in the document to a collection of \n",
    "sentences. Next we apply tokenizer to split the sentence. The output from the document assembler is a document. The input to sentence\n",
    "detector is the same document . The output of sentence detector is document but with sentence detection. This is fed as input\n",
    "to the tokenizer which outputs the tokens. This tokens are then passed to normalizer which gives the text only tokens by filtering\n",
    "tokens with special characters. This normalised tokens are then fed into the stemmer for stemming "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "val pipelineTwo = new Pipeline().setStages(Array(\n",
    "        documentAssembler,\n",
    "        sentenceDetector,\n",
    "        regexTokenizer,\n",
    "        normalizer,\n",
    "        stemmer\n",
    "    ))\n",
    "\n",
    "val result= pipelineTwo.\n",
    "    fit(mobilePatsDs).\n",
    "    transform(mobilePatsDs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code depicts the creation of pipeline for stemming of documents."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "val mobiltPatsDsStemmed= result.\n",
    "    withColumn(\"stemmedTokens\", col(\"stem.result\")).\n",
    "    select(\"stemmedTokens\",\"patent_id\",\"patnum\",\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stem tokens can be found in the field \"stem.result\" which is created as separate column in the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is tranformer called finisher that we think can be used for converting the annotation based output of the \n",
    "JNL pipeline into a typical spark based transformer pipeline. The finished has a property called setOutputAsArray which can\n",
    "be used for our advantage. This property is presumed to return array of tokens\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "val finisher = new Finisher()\n",
    "    .setInputCols(\"token\")\n",
    "    .setOutputAsArray(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stemmedTokens in the above dataframe can be used in the Pipeline with spark tranformers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
