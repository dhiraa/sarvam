{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title:  \"Dataset Handling\"\n",
    "description: \"Handling Dataset with TnsorFlow Dataset APIs\"\n",
    "excerpt: \"Handling Dataset with TnsorFlow Dataset APIs\"\n",
    "date:   2017-11-11\n",
    "mathjax: true\n",
    "comments: true \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jupyter notebook avaialble @ [https://github.com/dhiraa/tf-guru/blob/master/dataset/2017-11-11-DatasetHandling.ipynb](https://github.com/dhiraa/tf-guru/blob/master/dataset/2017-11-11-DatasetHandling.ipynb)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling TextDataset with TensorFlow APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing vocab list with TF APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Dynamic Sequence Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mageswarand/anaconda3/envs/tensorflow1.0/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import lookup\n",
    "from tensorflow.python.platform import gfile\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sequence_length(sequence_ids, axis=1, pad_word_id=0):\n",
    "    '''\n",
    "    Returns the sequence length, droping out all the padded tokens if the sequence is padded\n",
    "    \n",
    "    :param sequence_ids: Tensor(shape=[batch_size, doc_length])\n",
    "    :param pad_word_id: 0 is default\n",
    "    :return: Array of Document lengths of size batch_size\n",
    "    '''\n",
    "    flag = tf.greater(sequence_ids, pad_word_id)\n",
    "    used = tf.cast(flag, tf.int32)\n",
    "    length = tf.reduce_sum(used, axis)\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  0,  0,  0,  0,  0],\n",
       "       [ 3,  4,  0,  0,  0,  0],\n",
       "       [ 5,  6,  4,  0,  0,  0],\n",
       "       [ 7,  8,  6,  4,  0,  0],\n",
       "       [ 9, 10,  6, 11, 12, 13],\n",
       "       [ 0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MAX_DOC_LENGTHS = 4\n",
    "# rand_array = np.random.randint(1,MAX_DOC_LENGTHS, size=(3,5,4))\n",
    "\n",
    "#Assume all negative values are padding\n",
    "rand_array = np.array([[ 2,  0,  0,  0,  0,  0],\n",
    " [ 3,  4,  0,  0,  0,  0],\n",
    " [ 5,  6,  4,  0,  0,  0],\n",
    " [ 7,  8,  6,  4,  0,  0],\n",
    " [ 9, 10,  6, 11, 12, 13],\n",
    " [ 0,  0,  0,  0,  0,  0]])\n",
    "\n",
    "rand_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get dynamic sequence lengths:  [1 2 3 4 6 0]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "        length = get_sequence_length(rand_array, axis=1, pad_word_id=0)\n",
    "        print(\"Get dynamic sequence lengths: \", sess.run(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get dynamic sequence lengths:  [1 2 3 4 6 0]\n",
      "Get dynamic sequence lengths:  [[5 5 5 5]\n",
      " [5 5 5 5]\n",
      " [5 5 5 5]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "        length = get_sequence_length(rand_array, axis=1, pad_word_id=0)\n",
    "        print(\"Get dynamic sequence lengths: \", sess.run(length))\n",
    "        data = np.random.randint(1,6, size=(3,5,4))\n",
    "        length = get_sequence_length(data, axis=1)\n",
    "        print(\"Get dynamic sequence lengths: \", sess.run(length))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! rm vocab_test.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use tf.contrib.learn.preprocessing.VocabularyProcessor instead!\n",
    "def naive_vocab_creater(lines, out_file_name):\n",
    "    final_vocab = [\"<PAD>\", \"<UNK>\"]\n",
    "    vocab = [word for line in lines for word in line.split(\" \")]\n",
    "    vocab = set(vocab)\n",
    "\n",
    "    try:\n",
    "        vocab.remove(\"<UNK>\")\n",
    "    except:\n",
    "        print(\"No <UNK> token found\")\n",
    "\n",
    "    vocab = list(vocab)\n",
    "    final_vocab.extend(vocab)\n",
    "    return final_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assume each line to be an document\n",
    "lines = ['Some title', \n",
    "          'Simple',\n",
    "         'A longer title', \n",
    "         'An even longer title', \n",
    "         'This is longer than doc length isnt',\n",
    "          '']\n",
    "\n",
    "# Normally this takes the mean length of the words in the dataset documents\n",
    "MAX_DOCUMENT_LENGTH = 7\n",
    "#LSTM APIs have seq_length paramaters which can used to capture all the words and ignore\n",
    "#padding words\n",
    "\n",
    "# Padding word that is used when a document has less words than the calculated mean length of the words\n",
    "PADWORD = '<PAD>'\n",
    "PADWORD_ID = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version:  1.4.0\n",
      "15 words into vocab.tsv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: '<PAD>',\n",
       " 1: '<UNK>',\n",
       " 2: 'Some',\n",
       " 3: 'title',\n",
       " 4: 'Simple',\n",
       " 5: 'A',\n",
       " 6: 'longer',\n",
       " 7: 'An',\n",
       " 8: 'even',\n",
       " 9: 'This',\n",
       " 10: 'is',\n",
       " 11: 'than',\n",
       " 12: 'doc',\n",
       " 13: 'length',\n",
       " 14: 'isnt'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "print ('TensorFlow Version: ', tf.__version__)\n",
    "\n",
    "\n",
    "# Create vocabulary\n",
    "# min_frequency -> consider a word if and only it repeats for fiven count\n",
    "vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH, \n",
    "                                                                     min_frequency=0)\n",
    "vocab_processor.fit(lines)\n",
    "\n",
    "word_vocab = []\n",
    "\n",
    "#Create a file and store the words\n",
    "with gfile.Open('vocab_test.tsv', 'wb') as f:\n",
    "    f.write(\"{}\\n\".format(PADWORD))#, with latest LSTM APIs, sequence length for each document can be given\n",
    "    word_vocab.append(PADWORD)\n",
    "    for word, index in vocab_processor.vocabulary_._mapping.items():\n",
    "        word_vocab.append(word)\n",
    "        f.write(\"{}\\n\".format(word))\n",
    "        \n",
    "VOCAB_SIZE = len(vocab_processor.vocabulary_) + 1\n",
    "print ('{} words into vocab.tsv'.format(VOCAB_SIZE))\n",
    "\n",
    "EMBEDDING_SIZE = 3\n",
    "word_vocab = {id:word for id, word in enumerate(word_vocab)}\n",
    "word_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PAD>\r\n",
      "<UNK>\r\n",
      "Some\r\n",
      "title\r\n",
      "Simple\r\n",
      "A\r\n",
      "longer\r\n",
      "An\r\n",
      "even\r\n",
      "This\r\n",
      "is\r\n",
      "than\r\n",
      "doc\r\n",
      "length\r\n",
      "isnt\r\n"
     ]
    }
   ],
   "source": [
    "! cat vocab_test.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<': 2,\n",
       " '<P>': 0,\n",
       " '<U>': 1,\n",
       " '>': 3,\n",
       " 'A': 4,\n",
       " 'D': 5,\n",
       " 'K': 6,\n",
       " 'N': 7,\n",
       " 'P': 8,\n",
       " 'S': 9,\n",
       " 'T': 10,\n",
       " 'U': 11,\n",
       " 'a': 12,\n",
       " 'c': 13,\n",
       " 'd': 14,\n",
       " 'e': 15,\n",
       " 'g': 16,\n",
       " 'h': 17,\n",
       " 'i': 18,\n",
       " 'l': 19,\n",
       " 'm': 20,\n",
       " 'n': 21,\n",
       " 'o': 22,\n",
       " 'p': 23,\n",
       " 'r': 24,\n",
       " 's': 25,\n",
       " 't': 26,\n",
       " 'v': 27}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_chars_vocab = ['<P>', '<U>']\n",
    "\n",
    "def get_char_vocab(words_vocab):\n",
    "    '''\n",
    "\n",
    "    :param words_vocab: List of words\n",
    "    :return:\n",
    "    '''\n",
    "    chars = set()\n",
    "    for word in words_vocab:\n",
    "        for char in word:\n",
    "            chars.add(str(char))\n",
    "    return sorted(chars)\n",
    "\n",
    "words_chars_vocab.extend(get_char_vocab(word_vocab.values()))\n",
    "\n",
    "# Create char2id map\n",
    "char_2_id_map = {c:i for i,c in enumerate(words_chars_vocab)}\n",
    "\n",
    "CHAR_VOCAB_SIZE = len(char_2_id_map)\n",
    "char_2_id_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_char_ids = []\n",
    "char_ids_feature2 = []\n",
    "\n",
    "for line in lines:\n",
    "    for word in line.split():\n",
    "        word_2_char_ids = [char_2_id_map.get(c, 0) for c in word]\n",
    "        list_char_ids.append(word_2_char_ids)\n",
    "    char_ids_feature2.append(list_char_ids)\n",
    "    list_char_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[9, 22, 20, 15], [26, 18, 26, 19, 15]],\n",
       " [[9, 18, 20, 23, 19, 15]],\n",
       " [[4], [19, 22, 21, 16, 15, 24], [26, 18, 26, 19, 15]],\n",
       " [[4, 21], [15, 27, 15, 21], [19, 22, 21, 16, 15, 24], [26, 18, 26, 19, 15]],\n",
       " [[10, 17, 18, 25],\n",
       "  [18, 25],\n",
       "  [19, 22, 21, 16, 15, 24],\n",
       "  [26, 17, 12, 21],\n",
       "  [14, 22, 13],\n",
       "  [19, 15, 21, 16, 26, 17],\n",
       "  [18, 25, 21, 26]],\n",
       " []]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_ids_feature2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _pad_sequences(sequences, pad_tok, max_length):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sequences: a generator of list or tuple\n",
    "        pad_tok: the char to pad with\n",
    "\n",
    "    Returns:\n",
    "        a list of list where each sublist has same length\n",
    "    \"\"\"\n",
    "    sequence_padded, sequence_length = [], []\n",
    "\n",
    "    for seq in sequences:\n",
    "        seq = list(seq)\n",
    "        seq_ = seq[:max_length] + [pad_tok]*max(max_length - len(seq), 0)\n",
    "        sequence_padded +=  [seq_]\n",
    "        sequence_length += [min(len(seq), max_length)]\n",
    "\n",
    "    return sequence_padded, sequence_length\n",
    "\n",
    "\n",
    "def pad_sequences(sequences, pad_tok, nlevels, MAX_WORD_LENGTH=6):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sequences: a generator of list or tuple\n",
    "        pad_tok: the char to pad with\n",
    "        nlevels: \"depth\" of padding, for the case where we have characters ids\n",
    "\n",
    "    Returns:\n",
    "        a list of list where each sublist has same length\n",
    "\n",
    "    \"\"\"\n",
    "    if nlevels == 1:\n",
    "        sequence_padded = []\n",
    "        sequence_length = []\n",
    "        max_length = max(map(lambda x : len(x.split(\" \")), sequences))\n",
    "        # sequence_padded, sequence_length = _pad_sequences(sequences,\n",
    "        #                                                   pad_tok, max_length)\n",
    "        #breaking the code to pad the string instead on its ids\n",
    "        for seq in sequences:\n",
    "            current_length = len(seq.split(\" \"))\n",
    "            diff = max_length - current_length\n",
    "            pad_data = pad_tok * diff\n",
    "            sequence_padded.append(seq + pad_data)\n",
    "            sequence_length.append(max_length) #assumed\n",
    "\n",
    "        # print_info(sequence_length)\n",
    "    elif nlevels == 2:\n",
    "        # max_length_word = max([max(map(lambda x: len(x), seq))\n",
    "        #                        for seq in sequences])\n",
    "        sequence_padded, sequence_length = [], []\n",
    "        for seq in tqdm(sequences):\n",
    "            # all words are same length now\n",
    "            sp, sl = _pad_sequences(seq, pad_tok, MAX_WORD_LENGTH)\n",
    "            sequence_padded += [sp]\n",
    "            sequence_length += [sl]\n",
    "\n",
    "        max_length_sentence = max(map(lambda x : len(x), sequences))\n",
    "        sequence_padded, _ = _pad_sequences(sequence_padded,\n",
    "                                            [pad_tok]*MAX_WORD_LENGTH,\n",
    "                                            max_length_sentence) #TODO revert -1 to pad_tok\n",
    "        sequence_length, _ = _pad_sequences(sequence_length, 0,\n",
    "                                            max_length_sentence)\n",
    "\n",
    "    return sequence_padded, sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 27265.25it/s]\n"
     ]
    }
   ],
   "source": [
    "char_ids_feature2, char_seq_length = pad_sequences(char_ids_feature2, nlevels=2, pad_tok=0)\n",
    "char_ids_feature2 = np.array(char_ids_feature2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Some title',\n",
       " 'Simple',\n",
       " 'A longer title',\n",
       " 'An even longer title',\n",
       " 'This is longer than doc length isnt',\n",
       " '']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character IDs shape:  (6, 7, 6)\n",
      "Number of sentences:  6\n",
      "MAX_DOC_LENGTH:  7\n",
      "MAX_WORD_LENGTH:  8\n"
     ]
    }
   ],
   "source": [
    "print(\"Character IDs shape: \", char_ids_feature2.shape)\n",
    "print(\"Number of sentences: \", len(lines))\n",
    "print(\"MAX_DOC_LENGTH: \", max([len(line.split()) for line in lines]))\n",
    "print(\"MAX_WORD_LENGTH: \", 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# char_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WORD_EMBEDDING_SIZE = 3\n",
    "CHAR_EMBEDDING_SIZE = 3\n",
    "WORD_LEVEL_LSTM_HIDDEN_SIZE = 3\n",
    "CHAR_LEVEL_LSTM_HIDDEN_SIZE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple --> [4]\n",
      "Some unknown title --> [2 0 3]\n",
      "SparseTensor(indices=Tensor(\"StringSplit:0\", shape=(?, 2), dtype=int64), values=Tensor(\"StringSplit:1\", shape=(?,), dtype=string), dense_shape=Tensor(\"StringSplit:2\", shape=(2,), dtype=int64))\n",
      "INFO:tensorflow:encoded_sentence =====> Tensor(\"word_level_lstm_layer/concat:0\", shape=(?, ?, 6), dtype=float32)\n",
      "INFO:tensorflow:char_embeddings =====> Tensor(\"char_embed_layer/EmbedSequence/embedding_lookup:0\", shape=(6, 7, 6, 3), dtype=float32)\n",
      "INFO:tensorflow:reshaped char_embeddings =====> Tensor(\"chars_level_bilstm_layer/reduce_dimension_1:0\", shape=(?, ?, 3), dtype=float32)\n",
      "INFO:tensorflow:word_lengths =====> Tensor(\"chars_level_bilstm_layer/Sum:0\", shape=(?,), dtype=int32)\n",
      "INFO:tensorflow:encoded_words =====> Tensor(\"chars_level_bilstm_layer/Reshape:0\", shape=(?, ?, 6), dtype=float32)\n",
      "titles= [b'Some title' b'Simple' b'A longer title' b'An even longer title'\n",
      " b'This is longer than doc length isnt' b''] (6,)\n",
      "--------------------------------------------------------\n",
      "words= SparseTensorValue(indices=array([[0, 0],\n",
      "       [0, 1],\n",
      "       [1, 0],\n",
      "       [2, 0],\n",
      "       [2, 1],\n",
      "       [2, 2],\n",
      "       [3, 0],\n",
      "       [3, 1],\n",
      "       [3, 2],\n",
      "       [3, 3],\n",
      "       [4, 0],\n",
      "       [4, 1],\n",
      "       [4, 2],\n",
      "       [4, 3],\n",
      "       [4, 4],\n",
      "       [4, 5],\n",
      "       [4, 6]]), values=array([b'Some', b'title', b'Simple', b'A', b'longer', b'title', b'An',\n",
      "       b'even', b'longer', b'title', b'This', b'is', b'longer', b'than',\n",
      "       b'doc', b'length', b'isnt'], dtype=object), dense_shape=array([6, 7]))\n",
      "--------------------------------------------------------\n",
      "dense= [[b'Some' b'title' b'<PAD>' b'<PAD>' b'<PAD>' b'<PAD>' b'<PAD>']\n",
      " [b'Simple' b'<PAD>' b'<PAD>' b'<PAD>' b'<PAD>' b'<PAD>' b'<PAD>']\n",
      " [b'A' b'longer' b'title' b'<PAD>' b'<PAD>' b'<PAD>' b'<PAD>']\n",
      " [b'An' b'even' b'longer' b'title' b'<PAD>' b'<PAD>' b'<PAD>']\n",
      " [b'This' b'is' b'longer' b'than' b'doc' b'length' b'isnt']\n",
      " [b'<PAD>' b'<PAD>' b'<PAD>' b'<PAD>' b'<PAD>' b'<PAD>' b'<PAD>']] (?, ?)\n",
      "--------------------------------------------------------\n",
      "numbers= [[ 2  3  0  0  0  0  0]\n",
      " [ 4  0  0  0  0  0  0]\n",
      " [ 5  6  3  0  0  0  0]\n",
      " [ 7  8  6  3  0  0  0]\n",
      " [ 9 10  6 11 12 13 14]\n",
      " [ 0  0  0  0  0  0  0]] (?, ?)\n",
      "--------------------------------------------------------\n",
      "INFO:tensorflow:fw_output_one =====> (?, ?, 3)\n",
      "INFO:tensorflow:bw_output_one =====> (?, ?, 3)\n",
      "INFO:tensorflow:forward hidden state =====> (?, 3)\n",
      "INFO:tensorflow:forward out state =====> (?, 3)\n",
      "INFO:tensorflow:backward hidden state =====> (?, 3)\n",
      "INFO:tensorflow:backward out state =====> (?, 3)\n",
      "INFO:tensorflow:encoded_sentence =====> (?, ?, 6)\n",
      "INFO:tensorflow:encoded_senence_out =====> (6, 7, 6)\n",
      "Get dynamic sequence lengths:  [2 1 3 4 7 0]\n",
      "Word Ids: \n",
      " [[ 2  3  0  0  0  0  0]\n",
      " [ 4  0  0  0  0  0  0]\n",
      " [ 5  6  3  0  0  0  0]\n",
      " [ 7  8  6  3  0  0  0]\n",
      " [ 9 10  6 11 12 13 14]\n",
      " [ 0  0  0  0  0  0  0]]\n",
      "encoded_senence_out:\n",
      " [[[ 0.05938217  0.01118572  0.01365337 -0.01811065 -0.03092025  0.01489989]\n",
      "  [ 0.05038962 -0.09605373 -0.02682244 -0.02511055 -0.00465354 -0.03223317]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.00293788  0.03524039 -0.01051475  0.07168303  0.11795583 -0.01326659]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.02482312  0.05641751  0.01792154 -0.0489363  -0.0942255   0.0513152 ]\n",
      "  [ 0.00540489  0.11114837  0.05713746 -0.07931031 -0.09198114  0.02751064]\n",
      "  [ 0.0030675  -0.01884395 -0.00157457 -0.02511055 -0.00465354 -0.03223317]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.0078453   0.04857947  0.00845768 -0.03116864 -0.05025088  0.01930091]\n",
      "  [ 0.01671325 -0.03455263 -0.01158766 -0.06717809 -0.06650318  0.00306185]\n",
      "  [-0.01236045  0.06119553  0.03701975 -0.07931031 -0.09198114  0.02751064]\n",
      "  [-0.01604345 -0.05612635 -0.00998066 -0.02511055 -0.00465354 -0.03223317]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.04064208 -0.06444319  0.02884653 -0.07620216 -0.07278796  0.03816156]\n",
      "  [ 0.03554039 -0.11857731 -0.00489091  0.00794803  0.02129537 -0.00055451]\n",
      "  [ 0.00117065  0.00517926  0.04411225  0.02801108  0.02136863  0.03450494]\n",
      "  [ 0.01456851 -0.04704088 -0.0030652   0.08498491  0.12509167 -0.01813009]\n",
      "  [ 0.04645157 -0.02255858 -0.00319755  0.05201338  0.06316312  0.0125715 ]\n",
      "  [ 0.02177386  0.06271504  0.00906313  0.03278949  0.03740215  0.00463483]\n",
      "  [ 0.01551589  0.05452116 -0.00647796  0.03723343  0.0571533  -0.01325152]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]]]\n",
      "INFO:tensorflow:char_ids =====> (?, ?)\n",
      "INFO:tensorflow:char_ids_reshaped =====> [[ 9 22 20 15  0  0]\n",
      " [26 18 26 19 15  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 9 18 20 23 19 15]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 4  0  0  0  0  0]\n",
      " [19 22 21 16 15 24]\n",
      " [26 18 26 19 15  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 4 21  0  0  0  0]\n",
      " [15 27 15 21  0  0]\n",
      " [19 22 21 16 15 24]\n",
      " [26 18 26 19 15  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [10 17 18 25  0  0]\n",
      " [18 25  0  0  0  0]\n",
      " [19 22 21 16 15 24]\n",
      " [26 17 12 21  0  0]\n",
      " [14 22 13  0  0  0]\n",
      " [19 15 21 16 26 17]\n",
      " [18 25 21 26  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]]\n",
      "\n",
      "INFO:tensorflow:char_embeddings =====> (?, ?, 3)\n",
      "(42, 6, 3)\n",
      "INFO:tensorflow:word_lengths =====> [4 5 0 0 0 0 0 6 0 0 0 0 0 0 1 6 5 0 0 0 0 2 4 6 5 0 0 0 4 2 6 4 3 6 4 0 0\n",
      " 0 0 0 0 0]\n",
      "INFO:tensorflow:char_encoded =====> (?, ?, 6)\n",
      "char_encoded:\n",
      " [[[ 0.01229697  0.04309841 -0.10420672  0.04592736  0.04153579  0.00817145]\n",
      "  [ 0.00751953  0.01136758 -0.06272291 -0.02808281  0.00483301  0.02135517]\n",
      "  [ 0.03555843  0.01650089 -0.04150278 -0.02854454 -0.00650289  0.00651009]\n",
      "  [-0.00930316 -0.01440556  0.02546123 -0.05216182 -0.07760587 -0.0095683 ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[-0.05410502 -0.03717855 -0.08933543 -0.07512818 -0.10014518  0.1288799 ]\n",
      "  [-0.05824981 -0.03468876 -0.10110356 -0.07425629 -0.09713938  0.09198505]\n",
      "  [-0.11706582 -0.05491513 -0.16897558 -0.08470013 -0.12955432  0.0963521 ]\n",
      "  [-0.16038962 -0.06294608 -0.12245169 -0.08491183 -0.11956481  0.03854973]\n",
      "  [-0.16451286 -0.05156391 -0.02350709 -0.05216182 -0.07760587 -0.0095683 ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " ..., \n",
      " [[ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]]]\n",
      "INFO:tensorflow:char hidden_fw =====> (?, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:char output_fw =====> (?, 3)\n",
      "INFO:tensorflow:char hidden_bw =====> (?, 3)\n",
      "INFO:tensorflow:char output_bw =====> (?, 3)\n",
      "INFO:tensorflow:lstm_out_encoded_words =====> (?, 6)\n",
      "INFO:tensorflow:lstm_out_encoded_words =====> [[-0.00930316 -0.01440556  0.02546123  0.04592736  0.04153579  0.00817145]\n",
      " [-0.16451286 -0.05156391 -0.02350709 -0.07512818 -0.10014518  0.1288799 ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [-0.06948498 -0.02573795 -0.01167641  0.06477174  0.04810657  0.02968378]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [-0.02401877 -0.01998367  0.05892226 -0.04948393 -0.06492176 -0.01322675]\n",
      " [-0.026683    0.00296674 -0.04082637 -0.03328972 -0.05497397  0.04738995]\n",
      " [-0.16451286 -0.05156391 -0.02350709 -0.07512818 -0.10014518  0.1288799 ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [-0.00766473 -0.01684146  0.09730034 -0.06925613 -0.06960068 -0.02998063]\n",
      " [-0.02747298 -0.01944469  0.09574248 -0.09780926 -0.11798249 -0.02254665]\n",
      " [-0.026683    0.00296674 -0.04082637 -0.03328972 -0.05497397  0.04738995]\n",
      " [-0.16451286 -0.05156391 -0.02350709 -0.07512818 -0.10014518  0.1288799 ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [-0.05654893  0.00675702 -0.16982977  0.0556264   0.01134641  0.0997662 ]\n",
      " [ 0.00111628  0.02368112 -0.12734856  0.08003104  0.05162023  0.04552263]\n",
      " [-0.026683    0.00296674 -0.04082637 -0.03328972 -0.05497397  0.04738995]\n",
      " [-0.05890017 -0.01325193  0.02614187 -0.03067964 -0.05620409  0.10077976]\n",
      " [ 0.01903334 -0.00449377  0.05788806 -0.01688151 -0.00076403  0.00094708]\n",
      " [-0.09242901 -0.06028008 -0.10109018 -0.08242876 -0.11914585  0.04614791]\n",
      " [-0.04669749 -0.02733641 -0.09788826  0.04806058  0.02960191  0.05402618]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]]\n",
      "\n",
      "INFO:tensorflow:encoded_words =====> (?, ?, 6)\n",
      "INFO:tensorflow:encoded_words =====> [[[-0.00930316 -0.01440556  0.02546123  0.04592736  0.04153579  0.00817145]\n",
      "  [-0.16451286 -0.05156391 -0.02350709 -0.07512818 -0.10014518  0.1288799 ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[-0.06948498 -0.02573795 -0.01167641  0.06477174  0.04810657  0.02968378]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[-0.02401877 -0.01998367  0.05892226 -0.04948393 -0.06492176 -0.01322675]\n",
      "  [-0.026683    0.00296674 -0.04082637 -0.03328972 -0.05497397  0.04738995]\n",
      "  [-0.16451286 -0.05156391 -0.02350709 -0.07512818 -0.10014518  0.1288799 ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[-0.00766473 -0.01684146  0.09730034 -0.06925613 -0.06960068 -0.02998063]\n",
      "  [-0.02747298 -0.01944469  0.09574248 -0.09780926 -0.11798249 -0.02254665]\n",
      "  [-0.026683    0.00296674 -0.04082637 -0.03328972 -0.05497397  0.04738995]\n",
      "  [-0.16451286 -0.05156391 -0.02350709 -0.07512818 -0.10014518  0.1288799 ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[-0.05654893  0.00675702 -0.16982977  0.0556264   0.01134641  0.0997662 ]\n",
      "  [ 0.00111628  0.02368112 -0.12734856  0.08003104  0.05162023  0.04552263]\n",
      "  [-0.026683    0.00296674 -0.04082637 -0.03328972 -0.05497397  0.04738995]\n",
      "  [-0.05890017 -0.01325193  0.02614187 -0.03067964 -0.05620409  0.10077976]\n",
      "  [ 0.01903334 -0.00449377  0.05788806 -0.01688151 -0.00076403  0.00094708]\n",
      "  [-0.09242901 -0.06028008 -0.10109018 -0.08242876 -0.11914585  0.04614791]\n",
      "  [-0.04669749 -0.02733641 -0.09788826  0.04806058  0.02960191  0.05402618]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    " \n",
    "# can use the vocabulary to convert words to numbers\n",
    "table = lookup.index_table_from_file(\n",
    "  vocabulary_file='vocab_test.tsv', \n",
    "    num_oov_buckets=0, vocab_size=None, \n",
    "    default_value=PADWORD_ID) #id of <PAD> is 0\n",
    "\n",
    "word2ids = table.lookup(tf.constant(lines[1].split()))\n",
    "word2ids_1 = table.lookup(tf.constant(\"Some unknown title\".split()))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #Tables needs to be initialized before useing it\n",
    "    tf.tables_initializer().run()\n",
    "    print (\"{} --> {}\".format(lines[1], word2ids.eval()))\n",
    "    print (\"{} --> {}\".format(\"Some unknown title\", word2ids_1.eval()))\n",
    "    \n",
    "# string operations\n",
    "# Array of Docs -> Split it into Tokens/words \n",
    "#               -> Convert it into Dense Tensor apending PADWORD\n",
    "#               -> Table lookup \n",
    "#               -> Slice it to MAX_DOCUMENT_LENGTH\n",
    "titles = tf.constant(lines)\n",
    "words = tf.string_split(titles)\n",
    "print(words)\n",
    "\n",
    "densewords = tf.sparse_tensor_to_dense(words, default_value=PADWORD)\n",
    "numbers = table.lookup(densewords)\n",
    "\n",
    "##Following extrasteps are taken care by above 'table.lookup'\n",
    "# now pad out with zeros and then slice to constant length\n",
    "# padding = tf.constant([[0,0],[0,MAX_DOCUMENT_LENGTH]])\n",
    "# # this takes care of documents with zero length also\n",
    "# padded = tf.pad(numbers, padding)\n",
    "\n",
    "#if you wanted to clip the document MAX size then it can be done herwwwwwwe!\n",
    "# sliced = tf.slice(numbers, [0,0], [-1, MAX_DOCUMENT_LENGTH])\n",
    "\n",
    "seq_length= get_sequence_length(numbers)\n",
    "with tf.device('/cpu:0'), tf.name_scope(\"embed-layer\"):  \n",
    "\n",
    "    # layer to take the words and convert them into vectors (embeddings)\n",
    "    # This creates embeddings matrix of [n_words, EMBEDDING_SIZE] and then\n",
    "    # maps word indexes of the sequence into\n",
    "    # [batch_size, MAX_DOCUMENT_LENGTH, EMBEDDING_SIZE].\n",
    "    word_embeddings = tf.contrib.layers.embed_sequence(numbers,\n",
    "                                              vocab_size=VOCAB_SIZE,\n",
    "                                              embed_dim=WORD_EMBEDDING_SIZE,\n",
    "                                                   initializer=tf.contrib.layers.xavier_initializer(\n",
    "                                                                   seed=42))\n",
    "\n",
    "with  tf.name_scope(\"word_level_lstm_layer\"):\n",
    "    # Create a LSTM Unit cell with hidden size of EMBEDDING_SIZE.\n",
    "    d_rnn_cell_fw_one = tf.nn.rnn_cell.LSTMCell(WORD_LEVEL_LSTM_HIDDEN_SIZE,\n",
    "                                                state_is_tuple=True)\n",
    "    #https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMStateTuple\n",
    "    d_rnn_cell_bw_one = tf.nn.rnn_cell.LSTMCell(WORD_LEVEL_LSTM_HIDDEN_SIZE,\n",
    "                                                state_is_tuple=True)\n",
    "\n",
    "    (fw_output_one, bw_output_one), output_states = tf.nn.bidirectional_dynamic_rnn(\n",
    "        cell_fw=d_rnn_cell_fw_one,\n",
    "        cell_bw=d_rnn_cell_bw_one,\n",
    "        dtype=tf.float32,\n",
    "        sequence_length=seq_length,\n",
    "        inputs=word_embeddings,\n",
    "        scope=\"encod_sentence\")\n",
    "\n",
    "    # [BATCH_SIZE, MAX_SEQ_LENGTH, 2*WORD_LEVEL_LSTM_HIDDEN_SIZE) TODO check MAX_SEQ_LENGTH?\n",
    "    encoded_sentence = tf.concat([fw_output_one,\n",
    "                                  bw_output_one], axis=-1)\n",
    "\n",
    "    tf.logging.info('encoded_sentence =====> {}'.format(encoded_sentence))\n",
    "\n",
    "with tf.variable_scope(\"char_embed_layer\"):\n",
    "    \n",
    "        char_ids = tf.convert_to_tensor(char_ids_feature2, np.int64)\n",
    "        s = tf.shape(char_ids)\n",
    "        #remove pad words\n",
    "        char_ids_reshaped = tf.reshape(char_ids, shape=(s[0] * s[1], s[2])) #20 -> char dim\n",
    "        \n",
    "        char_embeddings = tf.contrib.layers.embed_sequence(char_ids,\n",
    "                                                           vocab_size=CHAR_VOCAB_SIZE,\n",
    "                                                           embed_dim=CHAR_EMBEDDING_SIZE,\n",
    "                                                           initializer=tf.contrib.layers.xavier_initializer(\n",
    "                                                               seed=42))\n",
    "\n",
    "        #[BATCH_SIZE, MAX_SEQ_LENGTH, MAX_WORD_LEGTH, CHAR_EMBEDDING_SIZE]\n",
    "\n",
    "        tf.logging.info('char_embeddings =====> {}'.format(char_embeddings))\n",
    "\n",
    "with tf.variable_scope(\"chars_level_bilstm_layer\"):\n",
    "        # put the time dimension on axis=1\n",
    "        shape = tf.shape(char_embeddings)\n",
    "\n",
    "        BATCH_SIZE = shape[0]\n",
    "        MAX_DOC_LENGTH = shape[1]\n",
    "        CHAR_MAX_LENGTH = shape[2]\n",
    "\n",
    "        # [BATCH_SIZE, MAX_SEQ_LENGTH, MAX_WORD_LEGTH, CHAR_EMBEDDING_SIZE]  ===>\n",
    "        #      [BATCH_SIZE * MAX_SEQ_LENGTH, MAX_WORD_LEGTH, CHAR_EMBEDDING_SIZE]\n",
    "        char_embeddings = tf.reshape(char_embeddings,\n",
    "                                     shape=[BATCH_SIZE * MAX_DOC_LENGTH, CHAR_MAX_LENGTH,\n",
    "                                            CHAR_EMBEDDING_SIZE],\n",
    "                                     name=\"reduce_dimension_1\")\n",
    "\n",
    "        tf.logging.info('reshaped char_embeddings =====> {}'.format(char_embeddings))\n",
    "\n",
    "        # word_lengths = get_sequence_length_old(char_embeddings) TODO working\n",
    "        word_lengths = get_sequence_length(char_ids_reshaped)\n",
    "\n",
    "        tf.logging.info('word_lengths =====> {}'.format(word_lengths))\n",
    "\n",
    "        # bi lstm on chars\n",
    "        cell_fw = tf.contrib.rnn.LSTMCell(CHAR_LEVEL_LSTM_HIDDEN_SIZE,\n",
    "                                          state_is_tuple=True)\n",
    "        cell_bw = tf.contrib.rnn.LSTMCell(CHAR_LEVEL_LSTM_HIDDEN_SIZE,\n",
    "                                          state_is_tuple=True)\n",
    "\n",
    "        _output = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw=cell_fw,\n",
    "            cell_bw=cell_bw,\n",
    "            dtype=tf.float32,\n",
    "            sequence_length=word_lengths,\n",
    "            inputs=char_embeddings,\n",
    "            scope=\"encode_words\")\n",
    "\n",
    "        # read and concat output\n",
    "        (char_fw_output_one, char_bw_output_one) , output_state = _output\n",
    "        ((hidden_fw, output_fw), (hidden_bw, output_bw)) = output_state\n",
    "        encoded_words = tf.concat([output_fw, output_bw], axis=-1)\n",
    "        \n",
    "        char_encoded = tf.concat([char_fw_output_one,\n",
    "                                  char_bw_output_one], axis=-1)\n",
    "        lstm_out_encoded_words = encoded_words\n",
    "        # [BATCH_SIZE, MAX_SEQ_LENGTH, WORD_EMBEDDING_SIZE]\n",
    "        encoded_words = tf.reshape(encoded_words,\n",
    "                                   shape=[BATCH_SIZE, MAX_DOC_LENGTH, 2 *\n",
    "                                          CHAR_LEVEL_LSTM_HIDDEN_SIZE])\n",
    "\n",
    "        tf.logging.info('encoded_words =====> {}'.format(encoded_words))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    tf.tables_initializer().run()\n",
    "    print (\"titles=\", titles.eval(), titles.shape)\n",
    "    print('--------------------------------------------------------')\n",
    "    print (\"words=\", words.eval())\n",
    "    print('--------------------------------------------------------')\n",
    "    print (\"dense=\", densewords.eval(), densewords.shape)\n",
    "    print('--------------------------------------------------------')\n",
    "    print (\"numbers=\", numbers.eval(), numbers.shape)\n",
    "    print('--------------------------------------------------------')\n",
    "#     print (\"padding=\", padding.eval(), padding.shape)\n",
    "#     print('--------------------------------------------------------')\n",
    "#     print (\"padded=\", padded.eval(), padded.shape)\n",
    "#     print('--------------------------------------------------------')\n",
    "#     print (\"sliced=\", sliced.eval(), sliced.shape)\n",
    "#     print('--------------------------------------------------------')\n",
    "    \n",
    "\n",
    "    # [?, self.MAX_DOCUMENT_LENGTH, self.EMBEDDING_SIZE]\n",
    "#     tf.logging.info('words_embed={}'.format(word_embeddings))\n",
    "    \n",
    "# #     tf.logging.info('words_embed={}'.format(word_embeddings.eval()))\n",
    "\n",
    "\n",
    "    \n",
    "    tf.logging.info('fw_output_one =====> {}'.format(fw_output_one.get_shape()))\n",
    "    tf.logging.info('bw_output_one =====> {}'.format(bw_output_one.get_shape()))\n",
    "    tf.logging.info('forward hidden state =====> {}'.format(output_states[0][0].get_shape()))\n",
    "    tf.logging.info('forward out state =====> {}'.format(output_states[0][1].get_shape()))\n",
    "    tf.logging.info('backward hidden state =====> {}'.format(output_states[1][0].get_shape()))\n",
    "    tf.logging.info('backward out state =====> {}'.format(output_states[1][1].get_shape()))\n",
    "    \n",
    "    \n",
    "    tf.logging.info('encoded_sentence =====> {}'.format(encoded_sentence.get_shape()))\n",
    "    encoded_senence_out =  encoded_sentence.eval()\n",
    "    tf.logging.info('encoded_senence_out =====> {}'.format(encoded_senence_out.shape))\n",
    "    \n",
    "    length = get_sequence_length(numbers)\n",
    "    print(\"Get dynamic sequence lengths: \", sess.run(length))\n",
    "    #By printing the output of LSTM we can see clearly that it only calculates for given\n",
    "    #sequence length and rest are appended with zeros\n",
    "    print(\"Word Ids: \\n\", numbers.eval())\n",
    "    print(\"encoded_senence_out:\\n\" , encoded_senence_out)\n",
    "    \n",
    "    \n",
    "    tf.logging.info('char_ids =====> {}'.format(char_ids_reshaped.get_shape()))\n",
    "    tf.logging.info('char_ids_reshaped =====> {}\\n'.format(char_ids_reshaped.eval()))\n",
    "    \n",
    "    tf.logging.info('char_embeddings =====> {}'.format(char_embeddings.shape))\n",
    "    char_embeddings_out = char_embeddings.eval()\n",
    "    print(char_embeddings_out.shape)\n",
    "    \n",
    "    tf.logging.info('word_lengths =====> {}'.format(word_lengths.eval()))\n",
    "    tf.logging.info('char_encoded =====> {}'.format(char_encoded.get_shape()))\n",
    "    print(\"char_encoded:\\n\", char_encoded.eval())\n",
    "    \n",
    "    tf.logging.info('char hidden_fw =====> {}'.format(hidden_fw.get_shape()))\n",
    "    tf.logging.info('char output_fw =====> {}'.format(output_fw.get_shape()))\n",
    "    tf.logging.info('char hidden_bw =====> {}'.format(hidden_bw.get_shape()))\n",
    "    tf.logging.info('char output_bw =====> {}'.format(output_bw.get_shape()))\n",
    "    \n",
    "    tf.logging.info('lstm_out_encoded_words =====> {}'.format(lstm_out_encoded_words.get_shape()))\n",
    "    tf.logging.info('lstm_out_encoded_words =====> {}\\n'.format(lstm_out_encoded_words.eval()))\n",
    "    \n",
    "    tf.logging.info('encoded_words =====> {}'.format(encoded_words.get_shape()))\n",
    "    tf.logging.info('encoded_words =====> {}\\n'.format(encoded_words.eval()))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Estimators Inputs\n",
    "- https://www.tensorflow.org/api_docs/python/tf/estimator/inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls ../../../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/boston_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f26e1b0ce10>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/boston_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 126708.0, step = 1\n",
      "INFO:tensorflow:global_step/sec: 148.62\n",
      "INFO:tensorflow:loss = 11384.6, step = 101 (0.678 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.339\n",
      "INFO:tensorflow:loss = 9351.81, step = 201 (0.452 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.878\n",
      "INFO:tensorflow:loss = 12143.7, step = 301 (0.450 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.909\n",
      "INFO:tensorflow:loss = 10205.2, step = 401 (0.455 sec)\n",
      "INFO:tensorflow:global_step/sec: 156.643\n",
      "INFO:tensorflow:loss = 10879.1, step = 501 (0.646 sec)\n",
      "INFO:tensorflow:global_step/sec: 115.252\n",
      "INFO:tensorflow:loss = 8110.94, step = 601 (0.868 sec)\n",
      "INFO:tensorflow:global_step/sec: 197.659\n",
      "INFO:tensorflow:loss = 7560.51, step = 701 (0.499 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.226\n",
      "INFO:tensorflow:loss = 8271.62, step = 801 (0.534 sec)\n",
      "INFO:tensorflow:global_step/sec: 222.864\n",
      "INFO:tensorflow:loss = 7931.87, step = 901 (0.448 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.268\n",
      "INFO:tensorflow:loss = 7178.96, step = 1001 (0.470 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.903\n",
      "INFO:tensorflow:loss = 8838.28, step = 1101 (0.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 200.417\n",
      "INFO:tensorflow:loss = 7826.87, step = 1201 (0.499 sec)\n",
      "INFO:tensorflow:global_step/sec: 196.4\n",
      "INFO:tensorflow:loss = 5050.69, step = 1301 (0.515 sec)\n",
      "INFO:tensorflow:global_step/sec: 154.767\n",
      "INFO:tensorflow:loss = 3872.17, step = 1401 (0.634 sec)\n",
      "INFO:tensorflow:global_step/sec: 148.364\n",
      "INFO:tensorflow:loss = 6946.71, step = 1501 (0.676 sec)\n",
      "INFO:tensorflow:global_step/sec: 229.637\n",
      "INFO:tensorflow:loss = 5879.3, step = 1601 (0.434 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.153\n",
      "INFO:tensorflow:loss = 4922.89, step = 1701 (0.462 sec)\n",
      "INFO:tensorflow:global_step/sec: 243.873\n",
      "INFO:tensorflow:loss = 7677.53, step = 1801 (0.414 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.671\n",
      "INFO:tensorflow:loss = 6666.38, step = 1901 (0.452 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.031\n",
      "INFO:tensorflow:loss = 4426.92, step = 2001 (0.470 sec)\n",
      "INFO:tensorflow:global_step/sec: 243.954\n",
      "INFO:tensorflow:loss = 7037.42, step = 2101 (0.410 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.501\n",
      "INFO:tensorflow:loss = 5615.54, step = 2201 (0.514 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.03\n",
      "INFO:tensorflow:loss = 4116.04, step = 2301 (0.520 sec)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-283046d2c3b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_input_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Evaluate loss over one epoch of test_set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow1.0/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow1.0/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m           \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow1.0/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    519\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow1.0/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    890\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    893\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow1.0/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow1.0/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow1.0/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow1.0/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow1.0/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow1.0/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow1.0/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow1.0/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "COLUMNS = [\"crim\", \"zn\", \"indus\", \"nox\", \"rm\", \"age\",\n",
    "           \"dis\", \"tax\", \"ptratio\", \"medv\"]\n",
    "FEATURES = [\"crim\", \"zn\", \"indus\", \"nox\", \"rm\",\n",
    "            \"age\", \"dis\", \"tax\", \"ptratio\"]\n",
    "LABEL = \"medv\"\n",
    "\n",
    "\n",
    "def get_input_fn(data_set, num_epochs=None, shuffle=True):\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "        x=pd.DataFrame({k: data_set[k].values for k in FEATURES}),\n",
    "        y=pd.Series(data_set[LABEL].values),\n",
    "        num_epochs=num_epochs,\n",
    "        shuffle=shuffle)\n",
    "\n",
    "\n",
    "# def main(unused_argv):\n",
    "    # Load datasets\n",
    "training_set = pd.read_csv(\"../data/boston_train.csv\", skipinitialspace=True,\n",
    "                         skiprows=1, names=COLUMNS)\n",
    "test_set = pd.read_csv(\"../data/boston_test.csv\", skipinitialspace=True,\n",
    "                     skiprows=1, names=COLUMNS)\n",
    "\n",
    "# Set of 6 examples for which to predict median house values\n",
    "prediction_set = pd.read_csv(\"../data/boston_predict.csv\", skipinitialspace=True,\n",
    "                           skiprows=1, names=COLUMNS)\n",
    "\n",
    "# Feature cols\n",
    "feature_cols = [tf.feature_column.numeric_column(k) for k in FEATURES]\n",
    "\n",
    "# Build 2 layer fully connected DNN with 10, 10 units respectively.\n",
    "regressor = tf.estimator.DNNRegressor(feature_columns=feature_cols,\n",
    "                                    hidden_units=[10, 10],\n",
    "                                    model_dir=\"/tmp/boston_model\")\n",
    "\n",
    "# Train\n",
    "regressor.train(input_fn=get_input_fn(training_set), steps=5000)\n",
    "\n",
    "# Evaluate loss over one epoch of test_set.\n",
    "ev = regressor.evaluate(\n",
    "  input_fn=get_input_fn(test_set, num_epochs=1, shuffle=False))\n",
    "loss_score = ev[\"loss\"]\n",
    "print(\"Loss: {0:f}\".format(loss_score))\n",
    "\n",
    "# Print out predictions over a slice of prediction_set.\n",
    "y = regressor.predict(\n",
    "  input_fn=get_input_fn(prediction_set, num_epochs=1, shuffle=False))\n",
    "# .predict() returns an iterator of dicts; convert to a list and print\n",
    "# predictions\n",
    "predictions = list(p[\"predictions\"] for p in itertools.islice(y, 6))\n",
    "print(\"Predictions: {}\".format(str(predictions)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Char Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random_3_dim_mat = np.random.randint(1, 5, size=(3,5,4))\n",
    "\n",
    "#PADDED LENGTH => 5,5,5\n",
    "#ACtual LENGTH => 4,3,5\n",
    "random_3_dim_mat = np.array([\n",
    "        [\n",
    "        [1, 1, 2, 0],\n",
    "        [4, 0, 0, 0],\n",
    "        [2, 4, 2, 3],\n",
    "        [3, 1, 0, 0],\n",
    "        [0, 0, 0, 0]],\n",
    "\n",
    "       [[3, 1, 2, 1],\n",
    "        [1, 1, 2, 3],\n",
    "        [4, 2, 2, 1],\n",
    "        [0, 0, 0, 0], # eg: <PAD> ==> < P A D  > <PAD> <PAD> => 6, 12, 16, 19, 7, 0, 0\n",
    "        [0, 0, 0, 0]],\n",
    "\n",
    "       [[1, 3, 4, 2],\n",
    "        [4, 4, 3, 3],\n",
    "        [1, 2, 4, 2],\n",
    "        [4, 2, 2, 1],\n",
    "        [4, 2, 3, 2]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sequence_length(sequence_ids, axis=1, pad_word_id=0):\n",
    "    '''\n",
    "    Returns the sequence length, droping out all the padded tokens if the sequence is padded\n",
    "    \n",
    "    :param sequence_ids: Tensor(shape=[batch_size, doc_length])\n",
    "    :param pad_word_id: 0 is default\n",
    "    :return: Array of Document lengths of size batch_size\n",
    "    '''\n",
    "    flag = tf.greater_equal(sequence_ids, pad_word_id)\n",
    "    used = tf.cast(flag, tf.int32)\n",
    "    length = tf.reduce_sum(used, axis)\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 2, 0],\n",
       "       [4, 0, 0, 0],\n",
       "       [2, 4, 2, 3],\n",
       "       [3, 1, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [3, 1, 2, 1],\n",
       "       [1, 1, 2, 3],\n",
       "       [4, 2, 2, 1],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [1, 3, 4, 2],\n",
       "       [4, 4, 3, 3],\n",
       "       [1, 2, 4, 2],\n",
       "       [4, 2, 2, 1],\n",
       "       [4, 2, 3, 2]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# print(random_3_dim_mat)\n",
    "random_3_dim_mat_reshaped = random_3_dim_mat.reshape(15,4)\n",
    "random_3_dim_mat_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True False  True  True  True False False  True  True\n",
      "  True  True  True]\n",
      "[[5 5 5 5]\n",
      " [5 5 5 5]\n",
      " [5 5 5 5]]\n",
      "[[1 1 2 0]\n",
      " [4 0 0 0]\n",
      " [2 4 2 3]\n",
      " [3 1 0 0]\n",
      " [3 1 2 1]\n",
      " [1 1 2 3]\n",
      " [4 2 2 1]\n",
      " [1 3 4 2]\n",
      " [4 4 3 3]\n",
      " [1 2 4 2]\n",
      " [4 2 2 1]\n",
      " [4 2 3 2]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    find_zero_rows = tf.reduce_sum(random_3_dim_mat_reshaped, 1)\n",
    "    flag_zero_rows = tf.greater_equal(find_zero_rows, 1)\n",
    "    print(sess.run(flag_zero_rows))\n",
    "    filtered = tf.boolean_mask(random_3_dim_mat_reshaped, flag_zero_rows)\n",
    "\n",
    "#     flag = tf.greater_equal(random_3_dim_mat, 1)\n",
    "# #     print(flag.eval())\n",
    "#     used = tf.cast(flag, tf.int32)\n",
    "#     print(used.eval())\n",
    "#     length = tf.reduce_sum(used, 1)\n",
    "#     length = length.eval()\n",
    "#     print(length)\n",
    "#     length = tf.slice(length, [0, 0], [3, 1])\n",
    "#     length = tf.cast(length, tf.int32)\n",
    "#     length = length.eval()\n",
    "#     print(length)\n",
    "    \n",
    "    length = sess.run(get_sequence_length(random_3_dim_mat, axis=1))\n",
    "    print(length)\n",
    "    filtered = sess.run(filtered)\n",
    "    print(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 4)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 1, 3, 1, 4, 1, 0, 2, 1, 4, 1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_dim = np.random.randint(0, 5, size=(3,4,1))\n",
    "three_dim.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TopKV2(values=array([[ 9,  8,  4],\n",
      "       [44, 10,  9]], dtype=int32), indices=array([[5, 3, 2],\n",
      "       [2, 0, 5]], dtype=int32))\n",
      "[array([[4],\n",
      "       [4],\n",
      "       [1]]), array([[3],\n",
      "       [4],\n",
      "       [4]]), array([[3],\n",
      "       [0],\n",
      "       [0]]), array([[2],\n",
      "       [3],\n",
      "       [3]])]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    res = sess.run(tf.nn.top_k([[1,3,4,8,2,9], [10,3,44,8,2,9]], k=3))\n",
    "    unstacked = sess.run(tf.unstack(three_dim, axis=1))\n",
    "    print(res)\n",
    "    print(unstacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = res.indices\n",
    "# indices.append(res.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "col3 = indices[:, 2:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e', 'e']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x:\"e\", col3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References: \n",
    "- https://medium.com/towards-data-science/how-to-do-text-classification-using-tensorflow-word-embeddings-and-cnn-edae13b3e575\n",
    "- https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/blogs/textclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert this notebook for Docs\n",
    "! jupyter nbconvert --to markdown --output-dir ../docs/_posts 2017-11-11-DatasetHandling.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
