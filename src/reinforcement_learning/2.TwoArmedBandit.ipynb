{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "- https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Reinforcement learning provides the capacity for us not only to teach an artificial agent how to act, but to allow it to learn through it’s own interactions with an environment. By combining the complex representations that deep neural networks can learn with the goal-driven learning of an RL agent, computers have accomplished some amazing feats, like beating humans at over a dozen Atari games, and defeating the Go world champion.\n",
    "\n",
    "\n",
    "Learning how to build these agents requires a bit of a change in thinking for anyone used to working in a supervised learning setting though. Gone is the ability to simply get the algorithm to pair certain stimuli with certain responses. Instead RL algorithms must enable the agent to learn the correct pairings itself through the use of observations, rewards, and actions. Since there is no longer a “True” correct action for an agent to take in any given circumstance that we can just tell it, things get a little tricky. In this post and those to follow, I will be walking through the creation and training of reinforcement learning agents. The agent and task will begin simple, so that the concepts are clear, and then work up to more complex task and environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Armed Bandit\n",
    "\n",
    "The simplest reinforcement learning problem is the n-armed bandit. Essentially, there are n-many slot machines, each with a different fixed payout probability. The goal is to discover the machine with the best payout, and maximize the returned reward by always choosing it. We are going to make it even simpler, by only having two possible slot machines to choose between. In fact, this problem is so simple that it is more of a precursor to real RL problems than one itself. Let me explain. Typical aspects of a task that make it an RL problem are the following:\n",
    "- Different actions yield different rewards. For example, when looking for treasure in a maze, going left may lead to the treasure, whereas going right may lead to a pit of snakes.\n",
    "- Rewards are delayed over time. This just means that even if going left in the above example is the right things to do, we may not know it till later in the maze.\n",
    "- Reward for an action is conditional on the state of the environment. Continuing the maze example, going left may be ideal at a certain fork in the path, but not at others.\n",
    "\n",
    "The n-armed bandit is a nice starting place because we don’t have to worry about aspects #2 and #3. All we need to focus on is learning which rewards we get for each of the possible actions, and ensuring we chose the optimal ones.\n",
    "\n",
    "In the context of RL lingo, this is called learning a policy. We are going to be using a method called **policy gradients**, where our simple neural network learns a policy for picking actions by **adjusting it’s weights** through **gradient descent** using feedback from the environment. There is another approach to reinforcement learning where agents learn value functions. In those approaches, instead of learning the optimal action in a given state, the agent learns to predict how good a given state or action will be for the agent to be in. Both approaches allow agents to learn good behavior, but the policy gradient approach is a little more direct.\n",
    "\n",
    "### Policy Gradient\n",
    "The simplest way to think of a Policy gradient network is one which produces explicit outputs. In the case of our bandit, we don’t need to condition these outputs on any state. As such, our network will consist of just a set of weights, with each corresponding to each of the possible arms to pull in the bandit, and will represent how good our agent thinks it is to pull each arm. If we initialize these weights to 1, then our agent will be somewhat optimistic about each arm’s potential reward.\n",
    "\n",
    "To update our network, we will simply try an arm with an e-greedy policy. This means that most of the time our agent will choose the action that corresponds to the largest expected value, but occasionally, with $e$ probability, it will choose randomly. In this way, the agent can try out each of the different arms to continue to learn more about them. Once our agent has taken an action, it then receives a reward of either $1 or -1$. With this reward, we can then make an update to our network using the policy loss equation:\n",
    "\n",
    "$$Loss = -log(\\Pi)*A$$\n",
    "\n",
    "$A$ is advantage, and is an essential aspect of all reinforcement learning algorithms. Intuitively it corresponds to how much better an action was than some baseline. In future algorithms, we will develop more complex baselines to compare our rewards to, but for now we will assume that the baseline is 0, and it can be thought of as simply the reward we received for each action.\n",
    "\n",
    "$\\Pi$ is the policy. In this case, it corresponds to the chosen action’s weight.\n",
    "\n",
    "Intuitively, this loss function allows us to **increase the weight** for actions that yielded a **positive reward**, and **decrease** them for actions that yielded a **negative reward**. In this way the agent will be more or less likely to pick that action in the future. By taking actions, getting rewards, and updating our network in this circular manner, we will quickly converge to an agent that can solve our bandit problem! Don’t take my word for it though. Try it out yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0,\n",
       " -6.907755278982137,\n",
       " 0.6931471805599453,\n",
       " 1.0986122886681098,\n",
       " 2.302585092994046,\n",
       " 5.298317366548036)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "math.log(1), math.log(0.001), math.log(2), math.log(3), math.log(10), math.log(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Multi-armed bandit\n",
    "This tutorial contains a simple example of how to build a policy-gradient based agent that can solve the multi-armed bandit problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mageswarand/anaconda3/envs/tensorflow1.0/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bandits\n",
    "Here we define our bandits. For this example we are using a four-armed bandit. The pullBandit function generates a random number from a normal distribution with a mean of 0. The lower the bandit number, the more likely a positive reward will be returned. We want our agent to learn to always choose the bandit that will give that positive reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#List out our bandits. Currently bandit 4 (index#3) is set to most often provide a positive reward.\n",
    "#List out our bandit arms. \n",
    "#Currently arm 4 (index #3) is set to most often provide a positive reward.\n",
    "bandit_arms = [0.2,0,-0.2,-2] #the lesser the value the more the chances of getting higher value than that\n",
    "\n",
    "NUM_BANDITS = len(bandit_arms)\n",
    "\n",
    "def pullBandit(bandit):\n",
    "    #Get a random number.\n",
    "    result = np.random.randn(1)\n",
    "    if result > bandit:\n",
    "        #return a positive reward.\n",
    "        return 1\n",
    "    else:\n",
    "        #return a negative reward.\n",
    "        return -1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Agent   \n",
    "The code below established our simple neural agent. It consists of a set of values for each of the bandits. Each value is an estimate of the value of the return from choosing the bandit. We use a policy gradient method to update the agent by moving the value for the selected action toward the recieved reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#These two lines established the feed-forward part of the network. \n",
    "weights = tf.Variable(tf.ones([NUM_BANDITS]))\n",
    "output = tf.nn.softmax(weights)\n",
    "\n",
    "\n",
    "#The next six lines establish the training proceedure. We feed the reward and chosen action into the network\n",
    "#to compute the loss, and use it to update the network.\n",
    "reward_holder = tf.placeholder(shape=[1],dtype=tf.float32)\n",
    "action_holder = tf.placeholder(shape=[1],dtype=tf.int32)\n",
    "\n",
    "responsible_output = tf.slice(output,action_holder,[1])\n",
    "\n",
    "loss = -(tf.log(responsible_output)*reward_holder)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "update = optimizer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(4,) dtype=float32_ref>\n",
      "Tensor(\"Reshape_1:0\", shape=(4,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(weights)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Agent\n",
    "We will train our agent by taking actions in our environment, and recieving rewards. Using the rewards and actions, we can know how to properly update our network in order to more often choose actions that will yield the highest rewards over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Boltzmann_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.40000000000000002, 1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = [0.1,0.2,0.4,0.3]\n",
    "np.random.choice(tmp,p=tmp), np.argmax(tmp == np.random.choice(tmp,p=tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward for the 4 arms of the bandit: [ 1.  0.  0.  0.]\n",
      "Running reward for the 4 arms of the bandit: [  3.  -2.  10.  14.]\n",
      "Running reward for the 4 arms of the bandit: [  4.  -3.  14.  26.]\n",
      "Running reward for the 4 arms of the bandit: [ -2.   1.  14.  34.]\n",
      "Running reward for the 4 arms of the bandit: [ -8.  -8.  19.  42.]\n",
      "Running reward for the 4 arms of the bandit: [-16. -10.  18.  51.]\n",
      "Running reward for the 4 arms of the bandit: [-20.  -5.  20.  64.]\n",
      "Running reward for the 4 arms of the bandit: [-22. -17.  22.  72.]\n",
      "Running reward for the 4 arms of the bandit: [-23. -12.  28.  86.]\n",
      "Running reward for the 4 arms of the bandit: [ -22.  -13.   32.  100.]\n",
      "Running reward for the 4 arms of the bandit: [ -24.  -12.   35.  120.]\n",
      "Running reward for the 4 arms of the bandit: [ -25.   -9.   36.  139.]\n",
      "Running reward for the 4 arms of the bandit: [ -31.   -7.   46.  153.]\n",
      "Running reward for the 4 arms of the bandit: [ -33.   -3.   48.  169.]\n",
      "Running reward for the 4 arms of the bandit: [ -36.   -3.   54.  178.]\n",
      "Running reward for the 4 arms of the bandit: [ -38.   -3.   56.  188.]\n",
      "Running reward for the 4 arms of the bandit: [ -34.   -3.   60.  208.]\n",
      "Running reward for the 4 arms of the bandit: [ -36.   -1.   62.  226.]\n",
      "Running reward for the 4 arms of the bandit: [ -43.   -3.   62.  245.]\n",
      "Running reward for the 4 arms of the bandit: [ -47.   -5.   64.  263.]\n",
      "\n",
      "The agent thinks arm 4 is the most promising....\n",
      "...and it was right!\n"
     ]
    }
   ],
   "source": [
    "total_episodes = 1000 #Set total number of episodes to train agent on.\n",
    "total_reward = np.zeros(NUM_BANDITS) #Set scoreboard for bandit arms to 0.\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the tensorflow graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    while i < total_episodes:\n",
    "        \n",
    "        #Choose action according to Boltzmann distribution.\n",
    "        actions = sess.run(output)\n",
    "        a = np.random.choice(actions, p=actions)\n",
    "        action = np.argmax(actions == a) \n",
    "        #with \"a\" the randomness is assured though the probability of getting same index is high\n",
    "        #action = np.argmax(actions) #try this if you want\n",
    "\n",
    "        reward = pullBandit(bandit_arms[action]) #Get our reward from picking one of the bandit arms.\n",
    "        \n",
    "        #Update the network.\n",
    "        _,resp,ww = sess.run([update,responsible_output,weights], \n",
    "                             feed_dict={reward_holder:[reward],action_holder:[action]})\n",
    "        \n",
    "        #Update our running tally of scores.\n",
    "        total_reward[action] += reward\n",
    "        if i % 50 == 0:\n",
    "            print(\"Running reward for the \" + str(NUM_BANDITS) + \" arms of the bandit: \" + str(total_reward))\n",
    "        i+=1\n",
    "        \n",
    "        \n",
    "print(\"\\nThe agent thinks arm \" + str(np.argmax(ww)+1) + \" is the most promising....\")\n",
    "if np.argmax(ww) == np.argmax(-np.array(bandit_arms)):\n",
    "    print(\"...and it was right!\")\n",
    "else:\n",
    "    print(\"...and it was wrong!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets RIP the above code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmaxed:  [[ 0.0320586   0.64391422  0.23688281  0.08714432]]\n",
      "scaled:  [[-3.44018984 -0.44018975 -1.44018972 -2.44018984]]\n",
      "[[1 2 1 1 3]]\n",
      "dummy_samples:  [[2]]\n",
      "chosen_action index:  2\n",
      "chosen_action:  12\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    dummy_weigths = [[2., 5., 4., 3.]]\n",
    "    softmaxed = tf.nn.softmax(dummy_weigths)\n",
    "    scaled = tf.log(softmaxed)\n",
    "    print(\"softmaxed: \", softmaxed.eval())\n",
    "    print(\"scaled: \", scaled.eval())\n",
    "    \n",
    "    #Uses the scaled logits index as the input for sampling space\n",
    "    dummy_samples = tf.multinomial(scaled, 5)\n",
    "    print(dummy_samples.eval()) #always selects the maximum weight index \n",
    "    \n",
    "    dummy_samples = tf.multinomial(tf.log(tf.nn.softmax(dummy_weigths)), 1).eval()\n",
    "    print(\"dummy_samples: \", dummy_samples)\n",
    "    print(\"chosen_action index: \", dummy_samples[0][0])\n",
    "    \n",
    "    dummy_possible_actions = tf.convert_to_tensor([10, 11, 12, 13])  # indices for possible actions\n",
    "    print(\"chosen_action: \", dummy_possible_actions[dummy_samples[0][0]].eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
