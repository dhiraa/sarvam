{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "- https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Networks and Beyond\n",
    "While our ordinary Q-network was able to barely perform as well as the Q-Table in a simple game environment, Deep Q-Networks are much more capable. In order to transform an ordinary Q-Network into a DQN we will be making the following improvements:\n",
    "\n",
    "- Going from a single-layer network to a multi-layer convolutional network.\n",
    "- Implementing Experience Replay, which will allow our network to train itself using stored memories from it’s experience.\n",
    "- Utilizing a second “target” network, which we will use to compute target Q-values during our updates.\n",
    "\n",
    "\n",
    "It was these three innovations that allowed the [Google DeepMind team to achieve superhuman performance on dozens of Atari games using their DQN agent](http://www.davidqiu.com:8888/research/nature14236.pdf). We will be walking through each individual improvement, and showing how to implement it. We won’t stop there though. The pace of Deep Learning research is extremely fast, and the DQN of 2014 is no longer the most advanced agent around anymore. I will discuss two simple additional improvements to the DQN architecture, Double DQN and Dueling DQN, that allow for improved performance, stability, and faster training time. In the end we will have a network that can tackle a number of challenging Atari games, and we will demonstrate how to train the DQN to learn a basic navigation task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting from Q-Network to Deep Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../docs/assets/rl/deepq_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addition 1: Convolutional Layers\n",
    "Since our agent is going to be learning to play video games, it has to be able to make sense of the game’s screen output in a way that is at least similar to how humans or other intelligent animals are able to. Instead of considering each pixel independently, convolutional layers allow us to consider regions of an image, and maintain spatial relationships between the objects on the screen as we send information up to higher levels of the network. In this way, they act similarly to human receptive fields. Indeed there is a body of research showing that convolutional neural network learn representations that are similar to those of [the primate visual cortex](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003963). As such, they are ideal for the first few elements within our network.\n",
    "\n",
    "In Tensorflow, we can utilize the tf.contrib.layers.convolution2d function to easily create a convolutional layer. We write for function as follows:\n",
    "\n",
    "```\n",
    "convolution_layer = tf.contrib.layers.convolution2d(inputs,num_outputs,kernel_size,stride,padding)\n",
    "```\n",
    "\n",
    "Here num_outs refers to how many filters we would like to apply to the previous layer. kernel_size refers to how large a window we would like to slide over the previous layer. Stride refers to how many pixels we want to skip as we slide the window across the layer. Finally, padding refers to whether we want our window to slide over just the bottom layer (“VALID”) or add padding around it (“SAME”) in order to ensure that the convolutional layer has the same dimensions as the previous layer. For more information, see the Tensorflow documentation.\n",
    "\n",
    "### Addition 2: Experience Replay\n",
    "The second major addition to make DQNs work is Experience Replay. The basic idea is that by storing an agent’s experiences, and then randomly drawing batches of them to train the network, we can more robustly learn to perform well in the task. By keeping the experiences we draw random, we prevent the network from only learning about what it is immediately doing in the environment, and allow it to learn from a more varied array of past experiences. Each of these experiences are stored as a tuple of <state,action,reward,next state>. The Experience Replay buffer stores a fixed number of recent memories, and as new ones come in, old ones are removed. When the time comes to train, we simply draw a uniform batch of random memories from the buffer, and train our network with them. For our DQN, we will build a simple class that handles storing and retrieving memories.  \n",
    "\n",
    "### Addition 3: Separate Target Network \n",
    "The third major addition to the DQN that makes it unique is the utilization of a second network during the training procedure. This second network is used to generate the target-Q values that will be used to compute the loss for every action during training. Why not use just use one network for both estimations? The issue is that at every step of training, the Q-network’s values shift, and if we are using a constantly shifting set of values to adjust our network values, then the value estimations can easily spiral out of control. The network can become destabilized by falling into feedback loops between the target and estimated Q-values. In order to mitigate that risk, the target network’s weights are fixed, and only periodically or slowly updated to the primary Q-networks values. In this way training can proceed in a more stable manner.\n",
    "\n",
    "Instead of updating the target network periodically and all at once, we will be updating it frequently, but slowly. This technique was introduced in another [DeepMind paper](https://arxiv.org/pdf/1509.02971.pdf) earlier this year, where they found that it stabilized the training process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going Beyond DQN\n",
    "With the additions above, we have everything we need to replicate the DWN of 2014. But the world moves fast, and a number of improvements above and beyond the DQN architecture [described by DeepMind](http://www.davidqiu.com:8888/research/nature14236.pdf), have allowed for even greater performance and stability. Before training your new DQN on your favorite ATARI game, I would suggest checking the newer additions out. I will provide a description and some code for two of them: Double DQN, and Dueling DQN. Both are simple to implement, and by combining both techniques, we can achieve better performance with faster training times.\n",
    "\n",
    "### Double DQN\n",
    "The main intuition behind [Double DQN](http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847) is that the regular DQN often overestimates the Q-values of the potential actions to take in a given state. While this would be fine if all actions were always overestimates equally, there was reason to believe this wasn’t the case. You can easily imagine that if certain suboptimal actions regularly were given higher Q-values than optimal actions, the agent would have a hard time ever learning the ideal policy. In order to correct for this, the authors of DDQN paper propose a simple trick: instead of taking the max over Q-values when computing the target-Q value for our training step, we use our primary network to chose an action, and our target network to generate the target Q-value for that action. By decoupling the action choice from the target Q-value generation, we are able to substantially reduce the overestimation, and train faster and more reliably. Below is the new DDQN equation for updating the target value.\n",
    "\n",
    "$$\n",
    "Q-Target = \\gamma + \\gamma Q(s^`, a, \\theta), \\theta^`))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dueling DQN\n",
    "\n",
    "![](../docs/assets/rl/double_dqn.png)\n",
    "\n",
    "Above: Regular DQN with a single stream for Q-values. Below: Dueling DQN where the value and advantage are calculated separately and then combined only at the final layer into a Q value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to explain the reasoning behind the architecture changes that Dueling DQN makes, we need to first explain some a few additional reinforcement learning terms. The Q-values that we have been discussing so far correspond to how good it is to take a certain action given a certain state. This can be written as Q(s,a). This action given state can actually be decomposed into two more fundamental notions of value. The first is the value function V(s), which says simple how good it is to be in any given state. The second is the advantage function A(a), which tells how much better taking a certain action would be compared to the others. We can then think of Q as being the combination of V and A. More formally:\n",
    "\n",
    "$$\n",
    "Q(s,a) = V(s) + A(a)\n",
    "$$\n",
    "\n",
    "The goal of Dueling DQN is to have a network that separately computes the advantage and value functions, and combines them back into a single Q-function only at the final layer. It may seem somewhat pointless to do this at first glance. Why decompose a function that we will just put back together? The key to realizing the benefit is to appreciate that our reinforcement learning agent may not need to care about both value and advantage at any given time. For example: imagine sitting outside in a park watching the sunset. It is beautiful, and highly rewarding to be sitting there. No action needs to be taken, and it doesn’t really make sense to think of the value of sitting there as being conditioned on anything beyond the environmental state you are in. We can achieve more robust estimates of state value by decoupling it from the necessity of being attached to specific actions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "![](../docs/assets/rl/block_env_screenshot.png)\n",
    "\n",
    "Simple block-world environment. The goal is to move the blue block to the green block while avoiding the red block.\n",
    "\n",
    "Now that we have learned all the tricks to get the most out of our DQN, let’s actually try it on a game environment! While the DQN we have described above could learn ATARI games with enough training, getting the network to perform well on those games takes at least a day of training on a powerful machine. For educational purposes, I have built a simple game environment which our DQN learns to master in a couple hours on a moderately powerful machine. In the environment the agent controls a blue square, and the goal is to navigate to the green squares (reward +1) while avoiding the red squares (reward -1). At the start of each episode all squares are randomly placed within a 5x5 grid-world. The agent has 50 steps to achieve as large a reward as possible. Because they are randomly positioned, the agent needs to do more than simply learn a fixed path, as was the case in the FrozenLake environment from Tutorial 0. Instead the agent must learn a notion of spatial relationships between the blocks. And indeed, it is able to do just that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game environment outputs 84x84x3 color images, and uses function calls as similar to the OpenAI gym as possible. In doing so, it should be easy to modify this code to work on any of the OpenAI atari games. I encourage those with the time and computing resources necessary to try getting the agent to perform well in an ATARI game. The hyperparameters may need some tuning, but it is definitely possible. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mageswarand/anaconda3/envs/tensorflow1.0/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the game environment\n",
    "Feel free to adjust the size of the gridworld. Making it smaller provides an easier task for our DQN agent, while making the world larger increases the challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fba849299e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gridworld import gameEnv\n",
    "\n",
    "env = gameEnv(partial=False,size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is an example of a starting environment in our simple game. The agent controls the blue square, and can move up, down, left, or right. The goal is to move to the green square (for +1 reward) and avoid the red square (for -1 reward). The position of the three blocks is randomized every episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the network itself\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self,h_size):\n",
    "        #The network recieves a frame from the game, flattened into an array.\n",
    "        #It then resizes it and processes it through four convolutional layers.\n",
    "        self.scalarInput =  tf.placeholder(shape=[None,21168],dtype=tf.float32)\n",
    "        self.imageIn = tf.reshape(self.scalarInput,shape=[-1,84,84,3])\n",
    "        self.conv1 = slim.conv2d( \\\n",
    "            inputs=self.imageIn,num_outputs=32,kernel_size=[8,8],stride=[4,4],padding='VALID', biases_initializer=None)\n",
    "        self.conv2 = slim.conv2d( \\\n",
    "            inputs=self.conv1,num_outputs=64,kernel_size=[4,4],stride=[2,2],padding='VALID', biases_initializer=None)\n",
    "        self.conv3 = slim.conv2d( \\\n",
    "            inputs=self.conv2,num_outputs=64,kernel_size=[3,3],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        self.conv4 = slim.conv2d( \\\n",
    "            inputs=self.conv3,num_outputs=h_size,kernel_size=[7,7],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        \n",
    "        #We take the output from the final convolutional layer and split it into separate advantage and value streams.\n",
    "        self.streamAC,self.streamVC = tf.split(self.conv4,2,3)\n",
    "        self.streamA = slim.flatten(self.streamAC)\n",
    "        self.streamV = slim.flatten(self.streamVC)\n",
    "        xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "        self.AW = tf.Variable(xavier_init([h_size//2,env.actions]))\n",
    "        self.VW = tf.Variable(xavier_init([h_size//2,1]))\n",
    "        self.Advantage = tf.matmul(self.streamA,self.AW)\n",
    "        self.Value = tf.matmul(self.streamV,self.VW)\n",
    "        \n",
    "        #Then combine them together to get our final Q-values.\n",
    "        self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions,env.actions,dtype=tf.float32)\n",
    "        \n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
    "        \n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay¶\n",
    "This class allows us to store experies and sample then randomly to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self,size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer,size)),[size,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple function to resize our game frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processState(states):\n",
    "    return np.reshape(states,[21168])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions allow us to update the parameters of our target network with those of the primary network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateTargetGraph(tfVars,tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network\n",
    "Setting all the training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32 #How many experiences to use for each training step.\n",
    "update_freq = 4 #How often to perform a training step.\n",
    "y = .99 #Discount factor on the target Q-values\n",
    "startE = 1 #Starting chance of random action\n",
    "endE = 0.1 #Final chance of random action\n",
    "annealing_steps = 10000. #How many steps of training to reduce startE to endE.\n",
    "num_episodes = 10000 #How many episodes of game environment to train network with.\n",
    "pre_train_steps = 10000 #How many steps of random actions before training begins.\n",
    "max_epLength = 50 #The max allowed length of our episode.\n",
    "load_model = False #Whether to load a saved model.\n",
    "path = \"./dqn\" #The path to save our model to.\n",
    "h_size = 512 #The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "tau = 0.001 #Rate to update target network toward primary network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n",
      "500 2.4 1\n",
      "1000 2.2 1\n",
      "1500 2.5 1\n",
      "2000 2.5 1\n",
      "2500 2.4 1\n",
      "3000 2.1 1\n",
      "3500 1.5 1\n",
      "4000 3.3 1\n",
      "4500 2.4 1\n",
      "5000 0.7 1\n",
      "5500 2.9 1\n",
      "6000 0.9 1\n",
      "6500 1.1 1\n",
      "7000 2.8 1\n",
      "7500 3.2 1\n",
      "8000 3.2 1\n",
      "8500 1.3 1\n",
      "9000 1.4 1\n",
      "9500 1.4 1\n",
      "10000 1.5 1\n",
      "10500 2.3 0.9549999999999828\n",
      "11000 1.5 0.9099999999999655\n",
      "11500 3.8 0.8649999999999483\n",
      "12000 1.7 0.819999999999931\n",
      "12500 2.7 0.7749999999999138\n",
      "13000 1.0 0.7299999999998965\n",
      "13500 2.8 0.6849999999998793\n",
      "14000 2.2 0.639999999999862\n",
      "14500 2.2 0.5949999999998448\n",
      "15000 1.9 0.5499999999998275\n",
      "15500 2.6 0.5049999999998103\n",
      "16000 2.1 0.4599999999998177\n",
      "16500 1.7 0.41499999999982823\n",
      "17000 1.2 0.36999999999983874\n",
      "17500 3.3 0.32499999999984924\n",
      "18000 2.4 0.27999999999985975\n",
      "18500 1.2 0.23499999999986562\n",
      "19000 2.8 0.18999999999986225\n",
      "19500 1.8 0.14499999999985888\n",
      "20000 1.6 0.09999999999985551\n",
      "20500 2.9 0.09999999999985551\n",
      "21000 1.8 0.09999999999985551\n",
      "21500 3.1 0.09999999999985551\n",
      "22000 1.5 0.09999999999985551\n",
      "22500 5.0 0.09999999999985551\n",
      "23000 3.6 0.09999999999985551\n",
      "23500 2.7 0.09999999999985551\n",
      "24000 6.4 0.09999999999985551\n",
      "24500 5.4 0.09999999999985551\n",
      "25000 5.8 0.09999999999985551\n",
      "25500 5.6 0.09999999999985551\n",
      "26000 5.2 0.09999999999985551\n",
      "26500 7.5 0.09999999999985551\n",
      "27000 8.1 0.09999999999985551\n",
      "27500 5.5 0.09999999999985551\n",
      "28000 5.5 0.09999999999985551\n",
      "28500 7.4 0.09999999999985551\n",
      "29000 5.1 0.09999999999985551\n",
      "29500 9.1 0.09999999999985551\n",
      "30000 7.5 0.09999999999985551\n",
      "30500 6.8 0.09999999999985551\n",
      "31000 9.7 0.09999999999985551\n",
      "31500 7.2 0.09999999999985551\n",
      "32000 10.4 0.09999999999985551\n",
      "32500 12.1 0.09999999999985551\n",
      "33000 6.4 0.09999999999985551\n",
      "33500 10.0 0.09999999999985551\n",
      "34000 12.0 0.09999999999985551\n",
      "34500 10.0 0.09999999999985551\n",
      "35000 14.7 0.09999999999985551\n",
      "35500 13.4 0.09999999999985551\n",
      "36000 9.9 0.09999999999985551\n",
      "36500 12.1 0.09999999999985551\n",
      "37000 13.6 0.09999999999985551\n",
      "37500 15.9 0.09999999999985551\n",
      "38000 9.6 0.09999999999985551\n",
      "38500 15.0 0.09999999999985551\n",
      "39000 12.8 0.09999999999985551\n",
      "39500 13.4 0.09999999999985551\n",
      "40000 11.0 0.09999999999985551\n",
      "40500 14.7 0.09999999999985551\n",
      "41000 19.0 0.09999999999985551\n",
      "41500 17.0 0.09999999999985551\n",
      "42000 17.1 0.09999999999985551\n",
      "42500 14.0 0.09999999999985551\n",
      "43000 19.6 0.09999999999985551\n",
      "43500 19.4 0.09999999999985551\n",
      "44000 17.8 0.09999999999985551\n",
      "44500 13.6 0.09999999999985551\n",
      "45000 13.8 0.09999999999985551\n",
      "45500 19.6 0.09999999999985551\n",
      "46000 14.7 0.09999999999985551\n",
      "46500 15.6 0.09999999999985551\n",
      "47000 21.7 0.09999999999985551\n",
      "47500 15.1 0.09999999999985551\n",
      "48000 19.2 0.09999999999985551\n",
      "48500 18.8 0.09999999999985551\n",
      "49000 22.7 0.09999999999985551\n",
      "49500 19.6 0.09999999999985551\n",
      "50000 16.7 0.09999999999985551\n",
      "Saved Model\n",
      "50500 16.5 0.09999999999985551\n",
      "51000 19.8 0.09999999999985551\n",
      "51500 19.4 0.09999999999985551\n",
      "52000 21.3 0.09999999999985551\n",
      "52500 18.5 0.09999999999985551\n",
      "53000 21.9 0.09999999999985551\n",
      "53500 18.4 0.09999999999985551\n",
      "54000 21.7 0.09999999999985551\n",
      "54500 20.6 0.09999999999985551\n",
      "55000 20.0 0.09999999999985551\n",
      "55500 19.7 0.09999999999985551\n",
      "56000 21.2 0.09999999999985551\n",
      "56500 19.2 0.09999999999985551\n",
      "57000 22.5 0.09999999999985551\n",
      "57500 20.3 0.09999999999985551\n",
      "58000 19.8 0.09999999999985551\n",
      "58500 21.2 0.09999999999985551\n",
      "59000 23.6 0.09999999999985551\n",
      "59500 22.3 0.09999999999985551\n",
      "60000 19.6 0.09999999999985551\n",
      "60500 20.9 0.09999999999985551\n",
      "61000 21.7 0.09999999999985551\n",
      "61500 19.6 0.09999999999985551\n",
      "62000 19.6 0.09999999999985551\n",
      "62500 19.8 0.09999999999985551\n",
      "63000 19.9 0.09999999999985551\n",
      "63500 20.0 0.09999999999985551\n",
      "64000 19.9 0.09999999999985551\n",
      "64500 22.4 0.09999999999985551\n",
      "65000 21.2 0.09999999999985551\n",
      "65500 21.5 0.09999999999985551\n",
      "66000 23.3 0.09999999999985551\n",
      "66500 19.9 0.09999999999985551\n",
      "67000 23.3 0.09999999999985551\n",
      "67500 21.7 0.09999999999985551\n",
      "68000 21.2 0.09999999999985551\n",
      "68500 22.3 0.09999999999985551\n",
      "69000 22.2 0.09999999999985551\n",
      "69500 22.3 0.09999999999985551\n",
      "70000 23.4 0.09999999999985551\n",
      "70500 22.7 0.09999999999985551\n",
      "71000 21.8 0.09999999999985551\n",
      "71500 22.4 0.09999999999985551\n",
      "72000 22.3 0.09999999999985551\n",
      "72500 23.3 0.09999999999985551\n",
      "73000 22.9 0.09999999999985551\n",
      "73500 22.8 0.09999999999985551\n",
      "74000 22.5 0.09999999999985551\n",
      "74500 21.3 0.09999999999985551\n",
      "75000 22.7 0.09999999999985551\n",
      "75500 22.8 0.09999999999985551\n",
      "76000 22.6 0.09999999999985551\n",
      "76500 21.4 0.09999999999985551\n",
      "77000 21.9 0.09999999999985551\n",
      "77500 22.2 0.09999999999985551\n",
      "78000 20.9 0.09999999999985551\n",
      "78500 22.2 0.09999999999985551\n",
      "79000 21.1 0.09999999999985551\n",
      "79500 23.5 0.09999999999985551\n",
      "80000 22.7 0.09999999999985551\n",
      "80500 22.5 0.09999999999985551\n",
      "81000 22.8 0.09999999999985551\n",
      "81500 22.1 0.09999999999985551\n",
      "82000 20.2 0.09999999999985551\n",
      "82500 20.0 0.09999999999985551\n",
      "83000 22.6 0.09999999999985551\n",
      "83500 22.2 0.09999999999985551\n",
      "84000 22.8 0.09999999999985551\n",
      "84500 23.5 0.09999999999985551\n",
      "85000 20.7 0.09999999999985551\n",
      "85500 23.4 0.09999999999985551\n",
      "86000 23.0 0.09999999999985551\n",
      "86500 22.6 0.09999999999985551\n",
      "87000 21.1 0.09999999999985551\n",
      "87500 19.5 0.09999999999985551\n",
      "88000 23.4 0.09999999999985551\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0552c76cb1a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m                     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateModel\u001b[0m\u001b[0;34m,\u001b[0m                         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalarInput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainBatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargetQ\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtargetQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrainBatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                     \u001b[0mupdateTarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetOps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Update the target network toward the primary network.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0mrAll\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-d9b4e7287d9b>\u001b[0m in \u001b[0;36mupdateTarget\u001b[0;34m(op_holder, sess)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mupdateTarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_holder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mop_holder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow1.0/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow1.0/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow1.0/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow1.0/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow1.0/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size)\n",
    "targetQN = Qnetwork(h_size)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "#Set the rate of random action decrease. \n",
    "e = startE\n",
    "stepDrop = (startE - endE)/annealing_steps\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    for i in range(num_episodes):\n",
    "        episodeBuffer = experience_buffer()\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        s = processState(s)\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        #The Q-Network\n",
    "        while j < max_epLength: #If the agent takes longer than 200 moves to reach either of the blocks, end the trial.\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                a = np.random.randint(0,4)\n",
    "            else:\n",
    "                a = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:[s]})[0]\n",
    "            s1,r,d = env.step(a)\n",
    "            s1 = processState(s1)\n",
    "            total_steps += 1\n",
    "            episodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]),[1,5])) #Save the experience to our episode buffer.\n",
    "            \n",
    "            if total_steps > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "                \n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    trainBatch = myBuffer.sample(batch_size) #Get a random batch of experiences.\n",
    "                    #Below we perform the Double-DQN update to the target Q-values\n",
    "                    Q1 = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                    Q2 = sess.run(targetQN.Qout,feed_dict={targetQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                    end_multiplier = -(trainBatch[:,4] - 1)\n",
    "                    doubleQ = Q2[range(batch_size),Q1]\n",
    "                    targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n",
    "                    #Update the network with our target values.\n",
    "                    _ = sess.run(mainQN.updateModel, \\\n",
    "                        feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,0]),mainQN.targetQ:targetQ, mainQN.actions:trainBatch[:,1]})\n",
    "                    \n",
    "                    updateTarget(targetOps,sess) #Update the target network toward the primary network.\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            \n",
    "            if d == True:\n",
    "\n",
    "                break\n",
    "        \n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "        #Periodically save the model. \n",
    "        if i % 1000 == 0:\n",
    "            saver.save(sess,path+'/model-'+str(i)+'.ckpt')\n",
    "            print(\"Saved Model\")\n",
    "        if len(rList) % 10 == 0:\n",
    "            print(total_steps,np.mean(rList[-10:]), e)\n",
    "    saver.save(sess,path+'/model-'+str(i)+'.ckpt')\n",
    "print(\"Percent of succesful episodes: \" + str(sum(rList)/num_episodes) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking network learning\n",
    "Mean reward over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb9cb3bf780>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFk6GmKejXQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb9cb1dd908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rMat = np.resize(np.array(rList),[len(rList)//100,100])\n",
    "rMean = np.average(rMat,1)\n",
    "plt.plot(rMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
