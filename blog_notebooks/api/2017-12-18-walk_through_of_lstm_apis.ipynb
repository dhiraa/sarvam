{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title:  \"LSTM Tensorflow APIs\"\n",
    "description: \"A simple walk through on Tensorflow LSTM APIs\"\n",
    "excerpt: \"A simple walk through on Tensorflow LSTM APIs\"\n",
    "date:   2017-12-17\n",
    "mathjax: true\n",
    "comments: true \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Most of the code is self explanatory, if not comment, I will add notes wherever posible!!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Data and LSTM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are the high level data flow used in i-Tagger:\n",
    "1. Preprocessing the data\n",
    "2. Creating the vocab \n",
    "    - Word/Tokens vocab which includes **word -> id** & **id -> word**\n",
    "    - Char vocab which includes **char -> id** & **id -> char**\n",
    "3. Prepraing the features\n",
    "    - Padding words/characters with custome function (or) \n",
    "    - Using Tensorflow APIs\n",
    "4. Model\n",
    "    - Word Embeddings\n",
    "    - Word Level BiLSTM encoding ignoring the padded words\n",
    "    - Character Embedding\n",
    "    - Char Level BiLSTM encoding ignoring the padded characters\n",
    "\n",
    "\n",
    "## You cant initialize graph components twice, if you encounter error, restart the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mageswarand/anaconda3/envs/tensorflow1.0/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import lookup\n",
    "from tensorflow.python.platform import gfile\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "tf.reset_default_graph()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Normally this takes the mean length of the words in the dataset documents\n",
    "MAX_DOCUMENT_LENGTH = 7\n",
    "\n",
    "# Padding word that is used when a document has less words than the calculated mean length of the words\n",
    "PAD_WORD = '<PAD>'\n",
    "PAD_WORD_ID = 0\n",
    "\n",
    "PAD_CHAR = \"<P>\"\n",
    "PAD_CHAR_ID = 0\n",
    "\n",
    "UNKNOWN_WORD = \"<UNKNOWN>\"\n",
    "UNKNOWN_WORD_ID = 1\n",
    "\n",
    "UNKNOWN_CHAR = \"<U>\"\n",
    "UNKNOWN_CHAR_ID = 1\n",
    "\n",
    "EMBEDDING_SIZE = 3\n",
    "WORD_EMBEDDING_SIZE = 3\n",
    "CHAR_EMBEDDING_SIZE = 3\n",
    "WORD_LEVEL_LSTM_HIDDEN_SIZE = 3\n",
    "CHAR_LEVEL_LSTM_HIDDEN_SIZE = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MISC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  2.  3.]\n",
      " [ 4.  5.  6.  7.]\n",
      " [ 7.  7.  7.  7.]]\n"
     ]
    }
   ],
   "source": [
    "value = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "init = tf.constant_initializer(value)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() :\n",
    "    x = tf.get_variable('x', shape = [3, 4], initializer = init)\n",
    "    x.initializer.run()\n",
    "    print(x.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are places where \"tf.get_variable\" is used without a initializer, in which case the TF engine initializes it with default intializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.38060057  0.50141478  0.40662122  0.72093308 -0.29107267]\n",
      " [-0.2623179   0.30131912 -0.55878854  0.63164377 -0.46682838]\n",
      " [ 0.75094438  0.61107099 -0.59864926  0.54592323 -0.48808187]\n",
      " [-0.43495807 -0.13385278  0.19299984  0.512218    0.28124976]\n",
      " [ 0.29379117 -0.02249491  0.35649407  0.51151133 -0.64783204]]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    W = tf.get_variable(\"W\", dtype=tf.float32, shape=[5,5])\n",
    "    b = tf.get_variable(\"b\", shape=[12],  dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(W))\n",
    "    print(b.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Dynamic Sequence Lengths **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO make this independent of dimensions\n",
    "def get_sequence_length(sequence_ids, pad_word_id=0, axis=1):\n",
    "    '''\n",
    "    Returns the sequence length, droping out all the padded tokens if the sequence is padded\n",
    "    \n",
    "    :param sequence_ids: Tensor(shape=[batch_size, doc_length])\n",
    "    :param pad_word_id: 0 is default\n",
    "    :return: Array of Document lengths of size batch_size\n",
    "    '''\n",
    "    flag = tf.greater(sequence_ids, pad_word_id)\n",
    "    used = tf.cast(flag, tf.int32)\n",
    "    length = tf.reduce_sum(used, axis)\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  0,  0,  0,  0,  0],\n",
       "       [ 3,  4,  0,  0,  0,  0],\n",
       "       [ 5,  6,  4,  0,  0,  0],\n",
       "       [ 7,  8,  6,  4,  0,  0],\n",
       "       [ 9, 10,  6, 11, 12, 13],\n",
       "       [ 0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MAX_DOC_LENGTHS = 4\n",
    "# rand_array = np.random.randint(1,MAX_DOC_LENGTHS, size=(3,5,4))\n",
    "\n",
    "#Assume all negative values are padding\n",
    "rand_array = np.array([[ 2,  0,  0,  0,  0,  0],\n",
    " [ 3,  4,  0,  0,  0,  0],\n",
    " [ 5,  6,  4,  0,  0,  0],\n",
    " [ 7,  8,  6,  4,  0,  0],\n",
    " [ 9, 10,  6, 11, 12, 13],\n",
    " [ 0,  0,  0,  0,  0,  0]])\n",
    "\n",
    "rand_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get dynamic sequence lengths:  [1 2 3 4 6 0]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "        length = get_sequence_length(rand_array, axis=1, pad_word_id=PAD_WORD_ID)\n",
    "        print(\"Get dynamic sequence lengths: \", sess.run(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data = np.random.randint(1,6, size=(3,MAX_DOCUMENT_LENGTH,4))\n",
    "\n",
    "# print(data)\n",
    "# with tf.Session() as sess:\n",
    "#         length = get_sequence_length(data, axis=1)\n",
    "#         print(\"Get dynamic sequence lengths: \", sess.run(length))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _pad_sequences(sequences, pad_tok, max_length):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sequences: a generator of list or tuple\n",
    "        pad_tok: the char to pad with\n",
    "\n",
    "    Returns:\n",
    "        a list of list where each sublist has same length\n",
    "    \"\"\"\n",
    "    sequence_padded, sequence_length = [], []\n",
    "\n",
    "    for seq in sequences:\n",
    "        seq = list(seq)\n",
    "        seq_ = seq[:max_length] + [pad_tok]*max(max_length - len(seq), 0)\n",
    "        sequence_padded +=  [seq_]\n",
    "        sequence_length += [min(len(seq), max_length)]\n",
    "\n",
    "    return sequence_padded, sequence_length\n",
    "\n",
    "\n",
    "def pad_sequences(sequences, pad_tok, nlevels, MAX_WORD_LENGTH=6):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sequences: a generator of list or tuple\n",
    "        pad_tok: the char to pad with\n",
    "        nlevels: \"depth\" of padding, for the case where we have characters ids\n",
    "\n",
    "    Returns:\n",
    "        a list of list where each sublist has same length\n",
    "\n",
    "    \"\"\"\n",
    "    if nlevels == 1:\n",
    "        sequence_padded = []\n",
    "        sequence_length = []\n",
    "        max_length = max(map(lambda x : len(x.split(\" \")), sequences))\n",
    "        # sequence_padded, sequence_length = _pad_sequences(sequences,\n",
    "        #                                                   pad_tok, max_length)\n",
    "        #breaking the code to pad the string instead on its ids\n",
    "        for seq in sequences:\n",
    "            current_length = len(seq.split(\" \"))\n",
    "            diff = max_length - current_length\n",
    "            pad_data = pad_tok * diff\n",
    "            sequence_padded.append(seq + pad_data)\n",
    "            sequence_length.append(max_length) #assumed\n",
    "\n",
    "        # print_info(sequence_length)\n",
    "    elif nlevels == 2:\n",
    "        # max_length_word = max([max(map(lambda x: len(x), seq))\n",
    "        #                        for seq in sequences])\n",
    "        sequence_padded, sequence_length = [], []\n",
    "        for seq in tqdm(sequences):\n",
    "            # all words are same length now\n",
    "            sp, sl = _pad_sequences(seq, pad_tok, MAX_WORD_LENGTH)\n",
    "            sequence_padded += [sp]\n",
    "            sequence_length += [sl]\n",
    "\n",
    "        max_length_sentence = max(map(lambda x : len(x), sequences))\n",
    "        sequence_padded, _ = _pad_sequences(sequence_padded,\n",
    "                                            [pad_tok]*MAX_WORD_LENGTH,\n",
    "                                            max_length_sentence) #TODO revert -1 to pad_tok\n",
    "        sequence_length, _ = _pad_sequences(sequence_length, 0,\n",
    "                                            max_length_sentence)\n",
    "\n",
    "    return sequence_padded, sequence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Cleaning /Precrocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assume each line to be an document\n",
    "lines = ['Simple',\n",
    "         'Some title', \n",
    "         'A longer title', \n",
    "         'An even longer title', \n",
    "         'This is longer than doc length isnt',\n",
    "          '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extracting Vocab from the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! rm vocab_test.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version:  1.4.0\n",
      "15 words into vocab.tsv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: '<PAD>',\n",
       " 1: '<UNK>',\n",
       " 2: 'Simple',\n",
       " 3: 'Some',\n",
       " 4: 'title',\n",
       " 5: 'A',\n",
       " 6: 'longer',\n",
       " 7: 'An',\n",
       " 8: 'even',\n",
       " 9: 'This',\n",
       " 10: 'is',\n",
       " 11: 'than',\n",
       " 12: 'doc',\n",
       " 13: 'length',\n",
       " 14: 'isnt'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "print ('TensorFlow Version: ', tf.__version__)\n",
    "\n",
    "\n",
    "# Create vocabulary\n",
    "# min_frequency -> consider a word if and only it repeats for fiven count\n",
    "vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH, \n",
    "                                                                     min_frequency=0)\n",
    "vocab_processor.fit(lines)\n",
    "\n",
    "word_vocab = []\n",
    "\n",
    "#Create a file and store the words\n",
    "with gfile.Open('vocab_test.tsv', 'wb') as f:\n",
    "    f.write(\"{}\\n\".format(PAD_WORD))\n",
    "    word_vocab.append(PAD_WORD)\n",
    "    for word, index in vocab_processor.vocabulary_._mapping.items():\n",
    "        word_vocab.append(word)\n",
    "        f.write(\"{}\\n\".format(word))\n",
    "        \n",
    "VOCAB_SIZE = len(vocab_processor.vocabulary_) + 1\n",
    "print ('{} words into vocab.tsv'.format(VOCAB_SIZE))\n",
    "\n",
    "\n",
    "id_2_word = {id:word for id, word in enumerate(word_vocab)}\n",
    "id_2_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PAD>\r\n",
      "<UNK>\r\n",
      "Simple\r\n",
      "Some\r\n",
      "title\r\n",
      "A\r\n",
      "longer\r\n",
      "An\r\n",
      "even\r\n",
      "This\r\n",
      "is\r\n",
      "than\r\n",
      "doc\r\n",
      "length\r\n",
      "isnt\r\n"
     ]
    }
   ],
   "source": [
    "! cat vocab_test.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naive_vocab_creater(lines, out_file_name):\n",
    "    final_vocab = [\"<PAD>\", \"<UNK>\"]\n",
    "    vocab = [word for line in lines for word in line.split(\" \")]\n",
    "    vocab = set(vocab)\n",
    "\n",
    "    try:\n",
    "        vocab.remove(\"<UNK>\")\n",
    "    except:\n",
    "        print(\"No <UNK> token found\")\n",
    "\n",
    "    vocab = list(vocab)\n",
    "    final_vocab.extend(vocab)\n",
    "    return final_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<': 0,\n",
       " '>': 1,\n",
       " 'A': 2,\n",
       " 'D': 3,\n",
       " 'K': 4,\n",
       " 'N': 5,\n",
       " 'P': 6,\n",
       " 'S': 7,\n",
       " 'T': 8,\n",
       " 'U': 9,\n",
       " 'a': 10,\n",
       " 'c': 11,\n",
       " 'd': 12,\n",
       " 'e': 13,\n",
       " 'g': 14,\n",
       " 'h': 15,\n",
       " 'i': 16,\n",
       " 'l': 17,\n",
       " 'm': 18,\n",
       " 'n': 19,\n",
       " 'o': 20,\n",
       " 'p': 21,\n",
       " 'r': 22,\n",
       " 's': 23,\n",
       " 't': 24,\n",
       " 'v': 25}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_char_vocab(words_vocab):\n",
    "    '''\n",
    "\n",
    "    :param words_vocab: List of words\n",
    "    :return:\n",
    "    '''\n",
    "    words_chars_vocab = ['<P>', '<U>']\n",
    "    chars = set()\n",
    "    for word in words_vocab:\n",
    "        for char in word:\n",
    "            chars.add(str(char))\n",
    "    words_chars_vocab.extend(chars)\n",
    "    return sorted(chars)\n",
    "\n",
    "\n",
    "chars = get_char_vocab(word_vocab)\n",
    "# Create char2id map\n",
    "char_2_id_map = {c:i for i,c in enumerate(chars)}\n",
    "\n",
    "CHAR_VOCAB_SIZE = len(char_2_id_map)\n",
    "char_2_id_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparing the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_char_ids = []\n",
    "char_ids_feature2 = []\n",
    "\n",
    "for line in lines:\n",
    "    for word in line.split():\n",
    "        word_2_char_ids = [char_2_id_map.get(c, 0) for c in word]\n",
    "        list_char_ids.append(word_2_char_ids)\n",
    "    char_ids_feature2.append(list_char_ids)\n",
    "    list_char_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[7, 16, 18, 21, 17, 13]],\n",
       " [[7, 20, 18, 13], [24, 16, 24, 17, 13]],\n",
       " [[2], [17, 20, 19, 14, 13, 22], [24, 16, 24, 17, 13]],\n",
       " [[2, 19], [13, 25, 13, 19], [17, 20, 19, 14, 13, 22], [24, 16, 24, 17, 13]],\n",
       " [[8, 15, 16, 23],\n",
       "  [16, 23],\n",
       "  [17, 20, 19, 14, 13, 22],\n",
       "  [24, 15, 10, 19],\n",
       "  [12, 20, 11],\n",
       "  [17, 13, 19, 14, 24, 15],\n",
       "  [16, 23, 19, 24]],\n",
       " []]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_ids_feature2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 34192.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6, 7, 6)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_ids_feature2, char_seq_length = pad_sequences(char_ids_feature2, nlevels=2, pad_tok=0)\n",
    "char_ids_feature2 = np.array(char_ids_feature2)\n",
    "char_ids_feature2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 7, 16, 18, 21, 17, 13],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0]],\n",
       "\n",
       "       [[ 7, 20, 18, 13,  0,  0],\n",
       "        [24, 16, 24, 17, 13,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0]],\n",
       "\n",
       "       [[ 2,  0,  0,  0,  0,  0],\n",
       "        [17, 20, 19, 14, 13, 22],\n",
       "        [24, 16, 24, 17, 13,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0]],\n",
       "\n",
       "       [[ 2, 19,  0,  0,  0,  0],\n",
       "        [13, 25, 13, 19,  0,  0],\n",
       "        [17, 20, 19, 14, 13, 22],\n",
       "        [24, 16, 24, 17, 13,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0]],\n",
       "\n",
       "       [[ 8, 15, 16, 23,  0,  0],\n",
       "        [16, 23,  0,  0,  0,  0],\n",
       "        [17, 20, 19, 14, 13, 22],\n",
       "        [24, 15, 10, 19,  0,  0],\n",
       "        [12, 20, 11,  0,  0,  0],\n",
       "        [17, 13, 19, 14, 24, 15],\n",
       "        [16, 23, 19, 24,  0,  0]],\n",
       "\n",
       "       [[ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_ids_feature2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character IDs shape:  (6, 7, 6)\n",
      "Number of sentences:  6\n",
      "MAX_DOC_LENGTH:  7\n",
      "MAX_WORD_LENGTH:  8\n"
     ]
    }
   ],
   "source": [
    "print(\"Character IDs shape: \", char_ids_feature2.shape)\n",
    "print(\"Number of sentences: \", len(lines))\n",
    "print(\"MAX_DOC_LENGTH: \", max([len(line.split()) for line in lines]))\n",
    "print(\"MAX_WORD_LENGTH: \", 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference Link: https://www.tensorflow.org/api_docs/python/tf/contrib/lookup/index_table_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<PAD>',\n",
       " 1: '<UNK>',\n",
       " 2: 'Simple',\n",
       " 3: 'Some',\n",
       " 4: 'title',\n",
       " 5: 'A',\n",
       " 6: 'longer',\n",
       " 7: 'An',\n",
       " 8: 'even',\n",
       " 9: 'This',\n",
       " 10: 'is',\n",
       " 11: 'than',\n",
       " 12: 'doc',\n",
       " 13: 'length',\n",
       " 14: 'isnt'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_2_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# can use the vocabulary to convert words to numbers\n",
    "table = lookup.index_table_from_file(\n",
    "  vocabulary_file='vocab_test.tsv', \n",
    "    num_oov_buckets=0, \n",
    "    vocab_size=None, \n",
    "    default_value=UNKNOWN_WORD_ID) #id of <PAD> is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"hash_table_Lookup:0\", shape=(?, ?), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# string operations\n",
    "# Array of Docs -> Split it into Tokens/words \n",
    "#               -> Convert it into Dense Tensor apending PADWORD\n",
    "#               -> Table lookup \n",
    "#               -> Slice it to MAX_DOCUMENT_LENGTH\n",
    "data = tf.constant(lines)\n",
    "words = tf.string_split(data)\n",
    "\n",
    "densewords = tf.sparse_tensor_to_dense(words, default_value=PAD_WORD)\n",
    "word_ids = table.lookup(densewords)\n",
    "print(word_ids)\n",
    "\n",
    "\n",
    "##Following extrasteps are taken care by above 'table.lookup'\n",
    "# now pad out with zeros and then slice to constant length\n",
    "# padding = tf.constant([[0,0],[0,MAX_DOCUMENT_LENGTH]])\n",
    "# # this takes care of documents with zero length also\n",
    "# padded = tf.pad(word_ids, padding)\n",
    "\n",
    "#if you wanted to clip the document MAX size then it can be done here!\n",
    "# sliced = tf.slice(word_ids, [0,0], [-1, MAX_DOCUMENT_LENGTH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some title --> [3 4]\n",
      "Some unknown title --> [3 1 4]\n",
      "[[b'Simple' b'<PAD>' b'<PAD>' b'<PAD>' b'<PAD>' b'<PAD>' b'<PAD>']\n",
      " [b'Some' b'title' b'<PAD>' b'<PAD>' b'<PAD>' b'<PAD>' b'<PAD>']\n",
      " [b'A' b'longer' b'title' b'<PAD>' b'<PAD>' b'<PAD>' b'<PAD>']\n",
      " [b'An' b'even' b'longer' b'title' b'<PAD>' b'<PAD>' b'<PAD>']\n",
      " [b'This' b'is' b'longer' b'than' b'doc' b'length' b'isnt']\n",
      " [b'<PAD>' b'<PAD>' b'<PAD>' b'<PAD>' b'<PAD>' b'<PAD>' b'<PAD>']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word2ids = table.lookup(tf.constant(lines[1].split()))\n",
    "word2ids_1 = table.lookup(tf.constant(\"Some unknown title\".split()))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #Tables needs to be initialized before using it\n",
    "    tf.tables_initializer().run()\n",
    "    print (\"{} --> {}\".format(lines[1], word2ids.eval()))\n",
    "    print (\"{} --> {}\".format(\"Some unknown title\", word2ids_1.eval()))\n",
    "    print(sess.run(densewords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_length= get_sequence_length(word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  0  0  0  0  0  0]\n",
      " [ 3  4  0  0  0  0  0]\n",
      " [ 5  6  4  0  0  0  0]\n",
      " [ 7  8  6  4  0  0  0]\n",
      " [ 9 10  6 11 12 13 14]\n",
      " [ 0  0  0  0  0  0  0]]\n",
      "==========================\n",
      "[1 2 3 4 7 0]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "     #Tables needs to be initialized before using it\n",
    "    tf.tables_initializer().run()\n",
    "    print(sess.run(word_ids))\n",
    "    print(\"==========================\")\n",
    "    print(sess.run(seq_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'), tf.name_scope(\"embed-layer\"):  \n",
    "\n",
    "    # layer to take the words and convert them into vectors (embeddings)\n",
    "    # This creates embeddings matrix of [n_words, EMBEDDING_SIZE] and then\n",
    "    # maps word indexes of the sequence into\n",
    "    # [batch_size, MAX_DOCUMENT_LENGTH, EMBEDDING_SIZE].\n",
    "    word_embeddings = tf.contrib.layers.embed_sequence(word_ids,\n",
    "                                              vocab_size=VOCAB_SIZE,\n",
    "                                              embed_dim=WORD_EMBEDDING_SIZE,\n",
    "                                                   initializer=tf.contrib.layers.xavier_initializer(\n",
    "                                                                   seed=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_embeddings : [batch_size, MAX_DOCUMENT_LENGTH, EMBEDDING_SIZE]  (6, 7, 3)\n",
      "<===============================================>\n",
      "[[[-0.36255243 -0.44535902 -0.18911475]\n",
      "  [ 0.52223814  0.20485282  0.34100413]\n",
      "  [ 0.52223814  0.20485282  0.34100413]\n",
      "  [ 0.52223814  0.20485282  0.34100413]\n",
      "  [ 0.52223814  0.20485282  0.34100413]\n",
      "  [ 0.52223814  0.20485282  0.34100413]\n",
      "  [ 0.52223814  0.20485282  0.34100413]]\n",
      "\n",
      " [[ 0.2578851  -0.3242403   0.41261792]\n",
      "  [ 0.37403101  0.11017311 -0.57392919]\n",
      "  [ 0.52223814  0.20485282  0.34100413]\n",
      "  [ 0.52223814  0.20485282  0.34100413]\n",
      "  [ 0.52223814  0.20485282  0.34100413]\n",
      "  [ 0.52223814  0.20485282  0.34100413]\n",
      "  [ 0.52223814  0.20485282  0.34100413]]\n",
      "\n",
      " [[-0.29184508  0.00701374 -0.15982357]\n",
      "  [-0.52557528  0.54521036  0.37919033]\n",
      "  [ 0.37403101  0.11017311 -0.57392919]\n",
      "  [ 0.52223814  0.20485282  0.34100413]\n",
      "  [ 0.52223814  0.20485282  0.34100413]\n",
      "  [ 0.52223814  0.20485282  0.34100413]\n",
      "  [ 0.52223814  0.20485282  0.34100413]]\n",
      "\n",
      " [[-0.09862986  0.11739373 -0.18522915]\n",
      "  [ 0.08441174 -0.30454588  0.20182377]\n",
      "  [-0.52557528  0.54521036  0.37919033]\n",
      "  [ 0.37403101  0.11017311 -0.57392919]\n",
      "  [ 0.52223814  0.20485282  0.34100413]\n",
      "  [ 0.52223814  0.20485282  0.34100413]\n",
      "  [ 0.52223814  0.20485282  0.34100413]]\n",
      "\n",
      " [[-0.56889766 -0.55320239  0.53333259]\n",
      "  [ 0.15531081 -0.31604788  0.17497867]\n",
      "  [-0.52557528  0.54521036  0.37919033]\n",
      "  [ 0.4238075  -0.28343284 -0.2626003 ]\n",
      "  [-0.04067117 -0.24000475 -0.33476758]\n",
      "  [-0.08533373  0.50026119 -0.095366  ]\n",
      "  [ 0.24810773  0.07649624 -0.23006374]]\n",
      "\n",
      " [[ 0.52223814  0.20485282  0.34100413]\n",
      "  [ 0.52223814  0.20485282  0.34100413]\n",
      "  [ 0.52223814  0.20485282  0.34100413]\n",
      "  [ 0.52223814  0.20485282  0.34100413]\n",
      "  [ 0.52223814  0.20485282  0.34100413]\n",
      "  [ 0.52223814  0.20485282  0.34100413]\n",
      "  [ 0.52223814  0.20485282  0.34100413]]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "     #Tables needs to be initialized before using it\n",
    "    tf.tables_initializer().run()\n",
    "    print(\"word_embeddings : [batch_size, MAX_DOCUMENT_LENGTH, EMBEDDING_SIZE] \", sess.run(word_embeddings).shape)\n",
    "    print(\"<===============================================>\")\n",
    "    print(sess.run(word_embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:encoded_sentence =====> Tensor(\"word_level_lstm_layer/concat:0\", shape=(?, ?, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with  tf.name_scope(\"word_level_lstm_layer\"):\n",
    "    # Create a LSTM Unit cell with hidden size of EMBEDDING_SIZE.\n",
    "    d_rnn_cell_fw_one = tf.nn.rnn_cell.LSTMCell(WORD_LEVEL_LSTM_HIDDEN_SIZE,\n",
    "                                                state_is_tuple=True)\n",
    "    #https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMStateTuple\n",
    "    d_rnn_cell_bw_one = tf.nn.rnn_cell.LSTMCell(WORD_LEVEL_LSTM_HIDDEN_SIZE,\n",
    "                                                state_is_tuple=True)\n",
    "\n",
    "    (fw_output_one, bw_output_one), output_states = tf.nn.bidirectional_dynamic_rnn(\n",
    "        cell_fw=d_rnn_cell_fw_one,\n",
    "        cell_bw=d_rnn_cell_bw_one,\n",
    "        dtype=tf.float32,\n",
    "        sequence_length=seq_length,\n",
    "        inputs=word_embeddings,\n",
    "        scope=\"encod_sentence\")\n",
    "\n",
    "    # [BATCH_SIZE, MAX_SEQ_LENGTH, 2*WORD_LEVEL_LSTM_HIDDEN_SIZE) TODO check MAX_SEQ_LENGTH?\n",
    "    encoded_sentence = tf.concat([fw_output_one,\n",
    "                                  bw_output_one], axis=-1)\n",
    "\n",
    "    tf.logging.info('encoded_sentence =====> {}'.format(encoded_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:fw_output_one \t=====> (?, ?, 3)\n",
      "INFO:tensorflow:bw_output_one \t=====> (?, ?, 3)\n",
      "INFO:tensorflow:=================================================================\n",
      "INFO:tensorflow:forward hidden state \t=====> (?, 3)\n",
      "INFO:tensorflow:forward out state \t=====> (?, 3)\n",
      "INFO:tensorflow:backward hidden state \t=====> (?, 3)\n",
      "INFO:tensorflow:backward out state \t=====> (?, 3)\n",
      "INFO:tensorflow:=================================================================\n",
      "INFO:tensorflow:encoded_sentence \t=====> (?, ?, 6)\n",
      "INFO:tensorflow:=================================================================\n",
      "INFO:tensorflow:encoded_senence_out \t=====> (6, 7, 6)\n",
      "INFO:tensorflow:=================================================================\n",
      "encoded_senence_out:\n",
      " [[[-0.01976291 -0.00923283  0.07085376  0.01306459  0.03344805 -0.04371169]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[-0.04772531  0.11881609 -0.03189922 -0.03764752 -0.07253012  0.04396541]\n",
      "  [ 0.03150441  0.07321765  0.00564955 -0.08588977  0.0064748   0.05586476]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.00638387 -0.04680856  0.03509862 -0.00451156  0.07471974 -0.05366353]\n",
      "  [-0.01397579 -0.11594137 -0.04197412 -0.00452052  0.06904873 -0.04156622]\n",
      "  [ 0.07882241 -0.11152568 -0.01332429 -0.08588977  0.0064748   0.05586476]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.02174438 -0.0365928   0.01485884 -0.00045621  0.02587043 -0.01915273]\n",
      "  [-0.01144159  0.03617312  0.01070661  0.01835279 -0.00727936 -0.01345707]\n",
      "  [-0.03788993 -0.0718103  -0.05404014 -0.00452052  0.06904873 -0.04156622]\n",
      "  [ 0.05784972 -0.06690629 -0.01731192 -0.08588977  0.0064748   0.05586476]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[-0.10292543  0.03962895  0.03830826  0.10795461 -0.02236171 -0.08061882]\n",
      "  [-0.11615913  0.11212726  0.0149015  -0.00817155 -0.02850961 -0.01442205]\n",
      "  [-0.12170991 -0.030709   -0.05609156 -0.02677963  0.04574642 -0.05044449]\n",
      "  [-0.05674711  0.04917112 -0.01982559 -0.1282053  -0.01708856  0.04558847]\n",
      "  [-0.02428005  0.03635133  0.03372234 -0.07728597  0.03857267  0.00894586]\n",
      "  [ 0.00910437 -0.04255038 -0.00274513 -0.05090441  0.03855459  0.02097071]\n",
      "  [ 0.04807584 -0.03034803 -0.00613315 -0.04632962 -0.00689608  0.03440535]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "     #Tables needs to be initialized before using it\n",
    "    tf.tables_initializer().run()\n",
    "    \n",
    "    tf.logging.info('fw_output_one \\t=====> {}'.format(fw_output_one.get_shape()))\n",
    "    tf.logging.info('bw_output_one \\t=====> {}'.format(bw_output_one.get_shape()))\n",
    "    tf.logging.info(\"=================================================================\")\n",
    "    tf.logging.info('forward hidden state \\t=====> {}'.format(output_states[0][0].get_shape()))\n",
    "    tf.logging.info('forward out state \\t=====> {}'.format(output_states[0][1].get_shape()))\n",
    "    tf.logging.info('backward hidden state \\t=====> {}'.format(output_states[1][0].get_shape()))\n",
    "    tf.logging.info('backward out state \\t=====> {}'.format(output_states[1][1].get_shape()))\n",
    "    tf.logging.info(\"=================================================================\")\n",
    "    tf.logging.info('encoded_sentence \\t=====> {}'.format(encoded_sentence.get_shape()))\n",
    "    tf.logging.info(\"=================================================================\")\n",
    "    encoded_senence_out =  encoded_sentence.eval()\n",
    "    #check for zeros in the encoced sentence, where it omits padded words\n",
    "    tf.logging.info('encoded_senence_out \\t=====> {}'.format(encoded_senence_out.shape))\n",
    "    tf.logging.info(\"=================================================================\")\n",
    "    print(\"encoded_senence_out:\\n\" , encoded_senence_out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 7, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 7, 16, 18, 21, 17, 13],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0]],\n",
       "\n",
       "       [[ 7, 20, 18, 13,  0,  0],\n",
       "        [24, 16, 24, 17, 13,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0]],\n",
       "\n",
       "       [[ 2,  0,  0,  0,  0,  0],\n",
       "        [17, 20, 19, 14, 13, 22],\n",
       "        [24, 16, 24, 17, 13,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0]],\n",
       "\n",
       "       [[ 2, 19,  0,  0,  0,  0],\n",
       "        [13, 25, 13, 19,  0,  0],\n",
       "        [17, 20, 19, 14, 13, 22],\n",
       "        [24, 16, 24, 17, 13,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0]],\n",
       "\n",
       "       [[ 8, 15, 16, 23,  0,  0],\n",
       "        [16, 23,  0,  0,  0,  0],\n",
       "        [17, 20, 19, 14, 13, 22],\n",
       "        [24, 15, 10, 19,  0,  0],\n",
       "        [12, 20, 11,  0,  0,  0],\n",
       "        [17, 13, 19, 14, 24, 15],\n",
       "        [16, 23, 19, 24,  0,  0]],\n",
       "\n",
       "       [[ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(char_ids_feature2.shape)\n",
    "char_ids_feature2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:char_ids_reshaped =====> Tensor(\"char_embed_layer/Reshape:0\", shape=(?, ?), dtype=int64)\n",
      "INFO:tensorflow:char_embeddings =====> Tensor(\"char_embed_layer/EmbedSequence/embedding_lookup:0\", shape=(6, 7, 6, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"char_embed_layer\"):\n",
    "    \n",
    "        char_ids = tf.convert_to_tensor(char_ids_feature2, np.int64)\n",
    "        s = tf.shape(char_ids)\n",
    "        #remove pad words\n",
    "        char_ids_reshaped = tf.reshape(char_ids, shape=(s[0] * s[1], s[2])) #6 -> char dim\n",
    "        \n",
    "        char_embeddings = tf.contrib.layers.embed_sequence(char_ids,\n",
    "                                                           vocab_size=CHAR_VOCAB_SIZE,\n",
    "                                                           embed_dim=CHAR_EMBEDDING_SIZE,\n",
    "                                                           initializer=tf.contrib.layers.xavier_initializer(\n",
    "                                                               seed=42))\n",
    "        word_lengths = get_sequence_length(char_ids_reshaped)\n",
    "\n",
    "        #[BATCH_SIZE, MAX_SEQ_LENGTH, MAX_WORD_LEGTH, CHAR_EMBEDDING_SIZE]\n",
    "        tf.logging.info('char_ids_reshaped =====> {}'.format(char_ids_reshaped))\n",
    "        tf.logging.info('char_embeddings =====> {}'.format(char_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:char_ids =====> (?, ?)\n",
      "INFO:tensorflow:=================================================================\n",
      "INFO:tensorflow:char_ids_reshaped shape =====> (42, 6)\n",
      "\n",
      "INFO:tensorflow:char_ids_reshaped =====> [[ 7 16 18 21 17 13]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 7 20 18 13  0  0]\n",
      " [24 16 24 17 13  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 2  0  0  0  0  0]\n",
      " [17 20 19 14 13 22]\n",
      " [24 16 24 17 13  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 2 19  0  0  0  0]\n",
      " [13 25 13 19  0  0]\n",
      " [17 20 19 14 13 22]\n",
      " [24 16 24 17 13  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 8 15 16 23  0  0]\n",
      " [16 23  0  0  0  0]\n",
      " [17 20 19 14 13 22]\n",
      " [24 15 10 19  0  0]\n",
      " [12 20 11  0  0  0]\n",
      " [17 13 19 14 24 15]\n",
      " [16 23 19 24  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]]\n",
      "\n",
      "INFO:tensorflow:word_lengths =====> [6 0 0 0 0 0 0 4 5 0 0 0 0 0 1 6 5 0 0 0 0 2 4 6 5 0 0 0 4 2 6 4 3 6 4 0 0\n",
      " 0 0 0 0 0]\n",
      "\n",
      "INFO:tensorflow:=================================================================\n",
      "INFO:tensorflow:char_embeddings =====> (6, 7, 6, 3)\n",
      "INFO:tensorflow:=================================================================\n",
      "(6, 7, 6, 3)\n",
      "INFO:tensorflow:=================================================================\n",
      "char_embeddings_out shape\n",
      " (6, 7, 6, 3)\n",
      "char_embeddings_out \n",
      " [[[[-0.0777044   0.09248734 -0.14593068]\n",
      "   [-0.37224245 -0.19546646  0.22445828]\n",
      "   [ 0.06220943 -0.08555388  0.28730118]\n",
      "   [ 0.09594625  0.33095586 -0.30559146]\n",
      "   [ 0.33953965 -0.07726747  0.38769937]\n",
      "   [-0.06722921  0.39412504 -0.07513303]]\n",
      "\n",
      "  [[ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]\n",
      "\n",
      "  [[ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]\n",
      "\n",
      "  [[ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]\n",
      "\n",
      "  [[ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]\n",
      "\n",
      "  [[ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]\n",
      "\n",
      "  [[ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]]\n",
      "\n",
      "\n",
      " [[[-0.0777044   0.09248734 -0.14593068]\n",
      "   [-0.06029078  0.31312287  0.20676911]\n",
      "   [ 0.06220943 -0.08555388  0.28730118]\n",
      "   [-0.06722921  0.39412504 -0.07513303]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]\n",
      "\n",
      "  [[-0.21950163 -0.27822471  0.44486523]\n",
      "   [-0.37224245 -0.19546646  0.22445828]\n",
      "   [-0.21950163 -0.27822471  0.44486523]\n",
      "   [ 0.33953965 -0.07726747  0.38769937]\n",
      "   [-0.06722921  0.39412504 -0.07513303]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]\n",
      "\n",
      "  [[ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]\n",
      "\n",
      "  [[ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]\n",
      "\n",
      "  [[ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]\n",
      "\n",
      "  [[ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]\n",
      "\n",
      "  [[ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]]\n",
      "\n",
      "\n",
      " [[[-0.28563282 -0.35087106 -0.14899191]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]\n",
      "\n",
      "  [[ 0.33953965 -0.07726747  0.38769937]\n",
      "   [-0.06029078  0.31312287  0.20676911]\n",
      "   [ 0.38928831 -0.23052102 -0.0280644 ]\n",
      "   [ 0.19546884  0.06026673 -0.18125311]\n",
      "   [-0.06722921  0.39412504 -0.07513303]\n",
      "   [ 0.14145756  0.13300532  0.06176662]]\n",
      "\n",
      "  [[-0.21950163 -0.27822471  0.44486523]\n",
      "   [-0.37224245 -0.19546646  0.22445828]\n",
      "   [-0.21950163 -0.27822471  0.44486523]\n",
      "   [ 0.33953965 -0.07726747  0.38769937]\n",
      "   [-0.06722921  0.39412504 -0.07513303]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]\n",
      "\n",
      "  [[ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]\n",
      "\n",
      "  [[ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]\n",
      "\n",
      "  [[ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]\n",
      "\n",
      "  [[ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]]\n",
      "\n",
      "\n",
      " [[[-0.28563282 -0.35087106 -0.14899191]\n",
      "   [ 0.38928831 -0.23052102 -0.0280644 ]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]\n",
      "\n",
      "  [[-0.06722921  0.39412504 -0.07513303]\n",
      "   [-0.34800512 -0.36377969  0.4280228 ]\n",
      "   [-0.06722921  0.39412504 -0.07513303]\n",
      "   [ 0.38928831 -0.23052102 -0.0280644 ]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]\n",
      "\n",
      "  [[ 0.33953965 -0.07726747  0.38769937]\n",
      "   [-0.06029078  0.31312287  0.20676911]\n",
      "   [ 0.38928831 -0.23052102 -0.0280644 ]\n",
      "   [ 0.19546884  0.06026673 -0.18125311]\n",
      "   [-0.06722921  0.39412504 -0.07513303]\n",
      "   [ 0.14145756  0.13300532  0.06176662]]\n",
      "\n",
      "  [[-0.21950163 -0.27822471  0.44486523]\n",
      "   [-0.37224245 -0.19546646  0.22445828]\n",
      "   [-0.21950163 -0.27822471  0.44486523]\n",
      "   [ 0.33953965 -0.07726747  0.38769937]\n",
      "   [-0.06722921  0.39412504 -0.07513303]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]\n",
      "\n",
      "  [[ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]\n",
      "\n",
      "  [[ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]\n",
      "\n",
      "  [[ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]]\n",
      "\n",
      "\n",
      " [[[ 0.06650281 -0.23993301  0.15900457]\n",
      "   [ 0.31817758 -0.04265583 -0.45091799]\n",
      "   [-0.37224245 -0.19546646  0.22445828]\n",
      "   [-0.32428217 -0.32085785  0.26928455]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]\n",
      "\n",
      "  [[-0.37224245 -0.19546646  0.22445828]\n",
      "   [-0.32428217 -0.32085785  0.26928455]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]\n",
      "\n",
      "  [[ 0.33953965 -0.07726747  0.38769937]\n",
      "   [-0.06029078  0.31312287  0.20676911]\n",
      "   [ 0.38928831 -0.23052102 -0.0280644 ]\n",
      "   [ 0.19546884  0.06026673 -0.18125311]\n",
      "   [-0.06722921  0.39412504 -0.07513303]\n",
      "   [ 0.14145756  0.13300532  0.06176662]]\n",
      "\n",
      "  [[-0.21950163 -0.27822471  0.44486523]\n",
      "   [ 0.31817758 -0.04265583 -0.45091799]\n",
      "   [ 0.12235987 -0.24899472  0.13785499]\n",
      "   [ 0.38928831 -0.23052102 -0.0280644 ]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]\n",
      "\n",
      "  [[-0.03204235 -0.18908501 -0.26374283]\n",
      "   [-0.06029078  0.31312287  0.20676911]\n",
      "   [ 0.33389199 -0.22329932 -0.20688666]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]\n",
      "\n",
      "  [[ 0.33953965 -0.07726747  0.38769937]\n",
      "   [-0.06722921  0.39412504 -0.07513303]\n",
      "   [ 0.38928831 -0.23052102 -0.0280644 ]\n",
      "   [ 0.19546884  0.06026673 -0.18125311]\n",
      "   [-0.21950163 -0.27822471  0.44486523]\n",
      "   [ 0.31817758 -0.04265583 -0.45091799]]\n",
      "\n",
      "  [[-0.37224245 -0.19546646  0.22445828]\n",
      "   [-0.32428217 -0.32085785  0.26928455]\n",
      "   [ 0.38928831 -0.23052102 -0.0280644 ]\n",
      "   [-0.21950163 -0.27822471  0.44486523]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]]\n",
      "\n",
      "\n",
      " [[[ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]\n",
      "\n",
      "  [[ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]\n",
      "\n",
      "  [[ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]\n",
      "\n",
      "  [[ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]\n",
      "\n",
      "  [[ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]\n",
      "\n",
      "  [[ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]\n",
      "\n",
      "  [[ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]\n",
      "   [ 0.41143936  0.16139096  0.26865625]]]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "     #Tables needs to be initialized before using it\n",
    "    tf.tables_initializer().run()\n",
    "    tf.logging.info('char_ids =====> {}'.format(char_ids_reshaped.get_shape()))\n",
    "    tf.logging.info(\"=================================================================\")\n",
    "    res = char_ids_reshaped.eval()\n",
    "    tf.logging.info('char_ids_reshaped shape =====> {}\\n'.format(res.shape))\n",
    "    tf.logging.info('char_ids_reshaped =====> {}\\n'.format(res))\n",
    "    tf.logging.info('word_lengths =====> {}\\n'.format(word_lengths.eval()))\n",
    "    tf.logging.info(\"=================================================================\")\n",
    "    tf.logging.info('char_embeddings =====> {}'.format(char_embeddings.shape))\n",
    "    tf.logging.info(\"=================================================================\")\n",
    "    char_embeddings_out = char_embeddings.eval()\n",
    "    print(char_embeddings_out.shape)\n",
    "    tf.logging.info(\"=================================================================\")\n",
    "    char_embeddings_out = char_embeddings.eval()\n",
    "    print(\"char_embeddings_out shape\\n\", char_embeddings_out.shape)\n",
    "    print(\"char_embeddings_out \\n\", char_embeddings_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:reshaped char_embeddings =====> Tensor(\"char_embed_layer/EmbedSequence/embedding_lookup:0\", shape=(6, 7, 6, 3), dtype=float32)\n",
      "INFO:tensorflow:word_lengths =====> Tensor(\"chars_level_bilstm_layer/Sum:0\", shape=(?,), dtype=int32)\n",
      "INFO:tensorflow:encoded_words =====> Tensor(\"chars_level_bilstm_layer/Reshape:0\", shape=(?, ?, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"chars_level_bilstm_layer\"):\n",
    "        # put the time dimension on axis=1\n",
    "        shape = tf.shape(char_embeddings)\n",
    "\n",
    "        BATCH_SIZE = shape[0]\n",
    "        MAX_DOC_LENGTH = shape[1]\n",
    "        CHAR_MAX_LENGTH = shape[2]\n",
    "\n",
    "        # [BATCH_SIZE, MAX_SEQ_LENGTH, MAX_WORD_LEGTH, CHAR_EMBEDDING_SIZE]  ===>\n",
    "        #      [BATCH_SIZE * MAX_SEQ_LENGTH, MAX_WORD_LEGTH, CHAR_EMBEDDING_SIZE]\n",
    "        char_embeddings_reshaped = tf.reshape(char_embeddings,\n",
    "                                     shape=[BATCH_SIZE * MAX_DOC_LENGTH, CHAR_MAX_LENGTH,\n",
    "                                            CHAR_EMBEDDING_SIZE],\n",
    "                                     name=\"reduce_dimension_1\")\n",
    "\n",
    "        tf.logging.info('reshaped char_embeddings =====> {}'.format(char_embeddings))\n",
    "\n",
    "        # word_lengths = get_sequence_length_old(char_embeddings) TODO working\n",
    "        word_lengths = get_sequence_length(char_ids_reshaped)\n",
    "\n",
    "        tf.logging.info('word_lengths =====> {}'.format(word_lengths))\n",
    "\n",
    "        # bi lstm on chars\n",
    "        cell_fw = tf.contrib.rnn.LSTMCell(CHAR_LEVEL_LSTM_HIDDEN_SIZE,\n",
    "                                          state_is_tuple=True)\n",
    "        cell_bw = tf.contrib.rnn.LSTMCell(CHAR_LEVEL_LSTM_HIDDEN_SIZE,\n",
    "                                          state_is_tuple=True)\n",
    "\n",
    "        _output = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw=cell_fw,\n",
    "            cell_bw=cell_bw,\n",
    "            dtype=tf.float32,\n",
    "            sequence_length=word_lengths,\n",
    "            inputs=char_embeddings_reshaped,\n",
    "            scope=\"encode_words\")\n",
    "\n",
    "        # read and concat output\n",
    "        (char_fw_output_one, char_bw_output_one) , output_state = _output\n",
    "        ((hidden_fw, output_fw), (hidden_bw, output_bw)) = output_state\n",
    "        encoded_words = tf.concat([output_fw, output_bw], axis=-1)\n",
    "        \n",
    "        char_encoded = tf.concat([char_fw_output_one,\n",
    "                                  char_bw_output_one], axis=-1)\n",
    "        lstm_out_encoded_words = encoded_words\n",
    "        # [BATCH_SIZE, MAX_SEQ_LENGTH, WORD_EMBEDDING_SIZE]\n",
    "        encoded_words = tf.reshape(encoded_words,\n",
    "                                   shape=[BATCH_SIZE, MAX_DOC_LENGTH, 2 *\n",
    "                                          CHAR_LEVEL_LSTM_HIDDEN_SIZE])\n",
    "\n",
    "        tf.logging.info('encoded_words =====> {}'.format(encoded_words))\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:char_embeddings =====> (6, 7, 6, 3)\n",
      "INFO:tensorflow:char_encoded =====> (?, ?, 6)\n",
      "char_encoded:\n",
      " [[[ 0.01289693  0.03215854  0.0049089   0.02896848  0.01639003  0.01760693]\n",
      "  [-0.00175623 -0.0293929   0.0057649  -0.00010866 -0.02562934  0.02795034]\n",
      "  [-0.02921278 -0.06415376 -0.00687395 -0.00772599 -0.02570577 -0.00648446]\n",
      "  [-0.00780411  0.04103209 -0.00176386  0.03748152  0.03362147 -0.01367662]\n",
      "  [-0.05726055 -0.03375445 -0.02974403 -0.02291263 -0.04749316 -0.00541486]\n",
      "  [-0.06904165  0.03736177 -0.03910471  0.06656165  0.05170256  0.00199127]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " ..., \n",
      " [[ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]]]\n",
      "INFO:tensorflow:char_fw_output_one =====> (?, ?, 3)\n",
      "INFO:tensorflow:char_bw_output_one =====> (?, ?, 3)\n",
      "INFO:tensorflow:char hidden_fw =====> (?, 3)\n",
      "INFO:tensorflow:char output_fw =====> (?, 3)\n",
      "INFO:tensorflow:char hidden_bw =====> (?, 3)\n",
      "INFO:tensorflow:char output_bw =====> (?, 3)\n",
      "INFO:tensorflow:lstm_out_encoded_words =====> (?, 6)\n",
      "INFO:tensorflow:lstm_out_encoded_words =====> [[-0.06904165  0.03736177 -0.03910471  0.02896848  0.01639003  0.01760693]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [-0.08010477  0.04053386 -0.04397502  0.06615115  0.02842443  0.00285061]\n",
      " [-0.06824481 -0.03029683 -0.01961379 -0.07010335 -0.10903811  0.08128914]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.0522683  -0.0173936   0.0234098   0.00279257  0.01505784  0.00404977]\n",
      " [-0.05949131  0.03973037 -0.03993161 -0.05799981 -0.08668529 -0.00825493]\n",
      " [-0.06824481 -0.03029683 -0.01961379 -0.07010335 -0.10903811  0.08128914]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.06193589 -0.03623288  0.02355442 -0.04486501 -0.01258224  0.00184671]\n",
      " [-0.0254913  -0.01338712 -0.01496682  0.04904424  0.03080448  0.02878405]\n",
      " [-0.05949131  0.03973037 -0.03993161 -0.05799981 -0.08668529 -0.00825493]\n",
      " [-0.06824481 -0.03029683 -0.01961379 -0.07010335 -0.10903811  0.08128914]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.02549884 -0.0902979   0.0286562  -0.06195    -0.05064696  0.01262299]\n",
      " [-0.00716779 -0.09858887  0.01202889 -0.01120201 -0.03311117  0.05316196]\n",
      " [-0.05949131  0.03973037 -0.03993161 -0.05799981 -0.08668529 -0.00825493]\n",
      " [ 0.04839797 -0.04648409  0.01917223 -0.10114393 -0.06143828  0.0299458 ]\n",
      " [ 0.03395079  0.0052877   0.00589937 -0.00152577  0.01732808 -0.00922754]\n",
      " [ 0.03923708  0.01564591  0.00568556 -0.08271034 -0.05915968  0.00373549]\n",
      " [-0.00868067 -0.13859105  0.01685495 -0.07438852 -0.06129534  0.08159567]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]]\n",
      "\n",
      "INFO:tensorflow:=====================================================================\n",
      "INFO:tensorflow:encoded_words =====> (?, ?, 6)\n",
      "INFO:tensorflow:encoded_words =====> [[[-0.06904165  0.03736177 -0.03910471  0.02896848  0.01639003  0.01760693]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[-0.08010477  0.04053386 -0.04397502  0.06615115  0.02842443  0.00285061]\n",
      "  [-0.06824481 -0.03029683 -0.01961379 -0.07010335 -0.10903811  0.08128914]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.0522683  -0.0173936   0.0234098   0.00279257  0.01505784  0.00404977]\n",
      "  [-0.05949131  0.03973037 -0.03993161 -0.05799981 -0.08668529 -0.00825493]\n",
      "  [-0.06824481 -0.03029683 -0.01961379 -0.07010335 -0.10903811  0.08128914]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.06193589 -0.03623288  0.02355442 -0.04486501 -0.01258224  0.00184671]\n",
      "  [-0.0254913  -0.01338712 -0.01496682  0.04904424  0.03080448  0.02878405]\n",
      "  [-0.05949131  0.03973037 -0.03993161 -0.05799981 -0.08668529 -0.00825493]\n",
      "  [-0.06824481 -0.03029683 -0.01961379 -0.07010335 -0.10903811  0.08128914]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.02549884 -0.0902979   0.0286562  -0.06195    -0.05064696  0.01262299]\n",
      "  [-0.00716779 -0.09858887  0.01202889 -0.01120201 -0.03311117  0.05316196]\n",
      "  [-0.05949131  0.03973037 -0.03993161 -0.05799981 -0.08668529 -0.00825493]\n",
      "  [ 0.04839797 -0.04648409  0.01917223 -0.10114393 -0.06143828  0.0299458 ]\n",
      "  [ 0.03395079  0.0052877   0.00589937 -0.00152577  0.01732808 -0.00922754]\n",
      "  [ 0.03923708  0.01564591  0.00568556 -0.08271034 -0.05915968  0.00373549]\n",
      "  [-0.00868067 -0.13859105  0.01685495 -0.07438852 -0.06129534  0.08159567]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.          0.        ]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    tf.tables_initializer().run()\n",
    "    \n",
    "    tf.logging.info('char_embeddings =====> {}'.format(char_embeddings.shape))\n",
    "    \n",
    "    tf.logging.info('char_encoded =====> {}'.format(char_encoded.get_shape()))\n",
    "    print(\"char_encoded:\\n\", char_encoded.eval())\n",
    "    \n",
    "    tf.logging.info('char_fw_output_one =====> {}'.format(char_fw_output_one.get_shape()))\n",
    "    tf.logging.info('char_bw_output_one =====> {}'.format(char_bw_output_one.get_shape()))\n",
    "    \n",
    "    tf.logging.info('char hidden_fw =====> {}'.format(hidden_fw.get_shape()))\n",
    "    tf.logging.info('char output_fw =====> {}'.format(output_fw.get_shape()))\n",
    "    tf.logging.info('char hidden_bw =====> {}'.format(hidden_bw.get_shape()))\n",
    "    tf.logging.info('char output_bw =====> {}'.format(output_bw.get_shape()))\n",
    "    \n",
    "    tf.logging.info('lstm_out_encoded_words =====> {}'.format(lstm_out_encoded_words.get_shape()))\n",
    "    #check for zeros in the encoced words, where it omits padded characters\n",
    "    tf.logging.info('lstm_out_encoded_words =====> {}\\n'.format(lstm_out_encoded_words.eval()))\n",
    "    tf.logging.info('=====================================================================')\n",
    "    tf.logging.info('encoded_words =====> {}'.format(encoded_words.get_shape()))\n",
    "    tf.logging.info('encoded_words =====> {}\\n'.format(encoded_words.eval()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References: \n",
    "- https://medium.com/towards-data-science/how-to-do-text-classification-using-tensorflow-word-embeddings-and-cnn-edae13b3e575\n",
    "- https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/blogs/textclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook 2017-12-18-walk_through_of_lstm_apis.ipynb to markdown\n",
      "[NbConvertApp] Writing 58408 bytes to ../../docs/_posts/2017-12-18-walk_through_of_lstm_apis.md\n"
     ]
    }
   ],
   "source": [
    "# Convert this notebook for Docs\n",
    "! jupyter nbconvert --to markdown --output-dir ../../docs/_posts 2017-12-18-walk_through_of_lstm_apis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
