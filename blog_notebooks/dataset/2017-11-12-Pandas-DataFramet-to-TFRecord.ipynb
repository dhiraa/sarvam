{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title:  \"Pandas DataFrame to TFRecord\"\n",
    "description: \"Working with TensorFlow TFRecords\"\n",
    "excerpt: \"Working with TensorFlow TFRecords\"\n",
    "date:   2017-11-11\n",
    "mathjax: true\n",
    "comments: true \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jupyter notebook avaialble @ [https://github.com/dhiraa/tf-guru/blob/master/dataset/2017-11-12-Pandas-DataFramet-to-TFRecord.ipynb](https://github.com/dhiraa/tf-guru/blob/master/dataset/2017-11-12-Pandas-DataFramet-to-TFRecord.ipynb)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What we are gonna learn?:\n",
    "- Refer my previous note book  @ [https://dhiraa.github.io/tf-guru/TFRecord.html](https://dhiraa.github.io/tf-guru/TFRecord.html)\n",
    "- How to prepare text documents for large scala training with TensorFlow?\n",
    "- What is the use case?\n",
    "    - Sequence Tagging\n",
    "    - Dataset format is adopted from [https://www.clips.uantwerpen.be/conll2003/ner/](https://www.clips.uantwerpen.be/conll2003/ner/). This is very custome setup for CoNLL dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handy Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vocab(df: pd.DataFrame, text_col: str):\n",
    "    '''\n",
    "\n",
    "    :param df: Pandas DataFrame\n",
    "    :param text_col: Text Column\n",
    "    :return:\n",
    "    '''\n",
    "    vocab = set()\n",
    "\n",
    "    row_wise_tokens = df[text_col].str.split(\" \").values\n",
    "    try:\n",
    "        for row in row_wise_tokens:\n",
    "            for token in row:\n",
    "                vocab.add(token)\n",
    "    except:\n",
    "        print(df)\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_char_vocab(words_vocab):\n",
    "    '''\n",
    "\n",
    "    :param words_vocab: List of words\n",
    "    :return:\n",
    "    '''\n",
    "    chars = set()\n",
    "    for word in words_vocab:\n",
    "        for char in word:\n",
    "            chars.add(char)\n",
    "    return sorted(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_char_vocab_from_df(df: pd.DataFrame, text_col):\n",
    "    return get_char_vocab(get_vocab(df, text_col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation\n",
    "\n",
    "- Read the IOB formated text data into Pandas DataFrame\n",
    "- Mark each sentence ending with <END> with the help of `new line` available in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "line_seperator = \"<CUSTOM_END>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/conll-sample-dataset.txt\", \n",
    "                 delimiter=\" \", \n",
    "                 header=None, \n",
    "                 skip_blank_lines =False).fillna(line_seperator)\n",
    "columns = [\"word\", \"tag\"] #define columns\n",
    "df.columns = columns # atach it to the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jean</td>\n",
       "      <td>B-PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pierre</td>\n",
       "      <td>I-PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lives</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New</td>\n",
       "      <td>B-LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>York</td>\n",
       "      <td>I-LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;CUSTOM_END&gt;</td>\n",
       "      <td>&lt;CUSTOM_END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>European</td>\n",
       "      <td>B-ORG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word           tag\n",
       "0          Jean         B-PER\n",
       "1        Pierre         I-PER\n",
       "2         lives             O\n",
       "3            in             O\n",
       "4           New         B-LOC\n",
       "5          York         I-LOC\n",
       "6             .             O\n",
       "7  <CUSTOM_END>  <CUSTOM_END>\n",
       "8           The             O\n",
       "9      European         B-ORG"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df[columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_seq_pair(df: pd.DataFrame,\n",
    "                  word_col,\n",
    "                  tag_col,\n",
    "                  line_seperator=line_seperator):\n",
    "    #get the column values\n",
    "    sequences = df[word_col].values\n",
    "    labels = df[tag_col].values \n",
    "    \n",
    "    #extract char vocab, to build for chars id feature\n",
    "    chars_vocab = get_char_vocab_from_df(df, word_col)\n",
    "    chars_vocab = {k: v for v, k in enumerate(chars_vocab)}\n",
    "    print(\"Chars Vocab For reference!\")\n",
    "    print(chars_vocab)\n",
    "    \n",
    "    list_text = []\n",
    "    list_char_ids = []\n",
    "    list_tag = [] \n",
    "    \n",
    "    #[feature1 ,feature2, label]\n",
    "    sentence_feature1 = []\n",
    "    char_ids_feature2 = []\n",
    "    tag_label = []\n",
    "    \n",
    "    for word, tag in zip(sequences, labels):\n",
    "        if word != line_seperator: #collect the sequence data till new line\n",
    "            list_text.append(word)\n",
    "            list_char_ids.append([chars_vocab[c] for c in word])\n",
    "            list_tag.append(tag)\n",
    "        else: #when a new line encountered, make an example with feature and label\n",
    "            sentence_feature1.append(\" \".join(list_text))\n",
    "            char_ids_feature2.append(list_char_ids)\n",
    "            tag_label.append(\" \".join(list_tag))\n",
    "#             list_seq_pair.append([sentence_feature1, char_ids_feature2, tag_label])\n",
    "            \n",
    "            #Make the container empty\n",
    "            list_text = []\n",
    "            list_char_ids = []\n",
    "            list_tag = []\n",
    "    return sentence_feature1, char_ids_feature2, tag_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chars Vocab For reference!\n",
      "{'.': 0, '<': 1, '>': 2, 'A': 3, 'C': 4, 'D': 5, 'E': 6, 'F': 7, 'J': 8, 'M': 9, 'N': 10, 'O': 11, 'P': 12, 'S': 13, 'T': 14, 'U': 15, 'Y': 16, '_': 17, 'a': 18, 'c': 19, 'd': 20, 'e': 21, 'h': 22, 'i': 23, 'k': 24, 'l': 25, 'm': 26, 'n': 27, 'o': 28, 'p': 29, 'r': 30, 's': 31, 't': 32, 'u': 33, 'v': 34, 'w': 35}\n"
     ]
    }
   ],
   "source": [
    "preprocessed_dataset = make_seq_pair(df, \"word\", \"tag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_feature1, char_ids_feature2, tag_label = preprocessed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Jean Pierre lives in New York .',\n",
       " [[8, 21, 18, 27],\n",
       "  [12, 23, 21, 30, 30, 21],\n",
       "  [25, 23, 34, 21, 31],\n",
       "  [23, 27],\n",
       "  [10, 21, 35],\n",
       "  [16, 28, 30, 24],\n",
       "  [0]],\n",
       " 'B-PER I-PER O O B-LOC I-LOC O')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Example 1:\")\n",
    "sentence_feature1[0], char_ids_feature2[0], tag_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Jean Pierre lives in New York .\n",
      "Char Ids [[8, 21, 18, 27], [12, 23, 21, 30, 30, 21], [25, 23, 34, 21, 31], [23, 27], [10, 21, 35], [16, 28, 30, 24], [0]]\n",
      "NER Tags B-PER I-PER O O B-LOC I-LOC O\n"
     ]
    }
   ],
   "source": [
    "print(\"Text: {}\".format(sentence_feature1[0]))\n",
    "print(\"Char Ids {}\".format(char_ids_feature2[0]))\n",
    "print(\"NER Tags {}\".format(tag_label[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to convert above feature & label pairs into TF Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as TFRecordFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    \"\"\"Wrapper for inserting int64 features into Example proto.\"\"\"\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Wrapper for inserting float features into Example proto.\"\"\"\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Wrapper for inserting bytes features into Example proto.\"\"\"\n",
    "    if not isinstance(value, list):\n",
    "        value = [value.encode()]\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "\n",
    "def _validate_text(text):\n",
    "    \"\"\"If text is not str or unicode, then try to convert it to str.\"\"\"\n",
    "  \n",
    "    if isinstance(text, str):\n",
    "        return text\n",
    "    elif isinstance(text, unicode):\n",
    "        return text.encode('utf8', 'ignore')\n",
    "    else:\n",
    "        return str(text)\n",
    "    \n",
    "def make_example(text_sequence, char_ids, labels):\n",
    "    features = tf.train.Features(feature={\n",
    "        \"seq/text\" : _bytes_feature(_validate_text(text_sequence)), #tf.VarLenFeature(tf.int64)\n",
    "#         \"seq/char_ids\" : _int64_feature(char_ids),\n",
    "        \"seq/labels\" : _bytes_feature(labels) #tf.VarLenFeature(tf.int64)\n",
    "    })\n",
    "    \n",
    "    return tf.train.Example(features=features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_into_tfrecord(preprocessed_dataset):\n",
    "    with open(\"/tmp/dummy.tfrecord\", \"wb\") as fp:\n",
    "        \n",
    "        writer = tf.python_io.TFRecordWriter(fp.name)\n",
    "        #get each sequence along with its labels and write it to a file as Sequence Example.\n",
    "        for text_sequence, char_ids, labels in zip(*preprocessed_dataset):\n",
    "            example_sequence = make_example(text_sequence, char_ids, labels)\n",
    "            writer.write(example_sequence.SerializeToString())\n",
    "        writer.close()\n",
    "        print(\"Wrote to {}\".format(fp.name))\n",
    "        return fp.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote to /tmp/dummy.tfrecord\n"
     ]
    }
   ],
   "source": [
    "save_path = save_into_tfrecord(preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read back from TFRecordFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_from_tfrecord(filenames):\n",
    "    tfrecord_file_queue = tf.train.string_input_producer(filenames, name='queue')\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, tfrecord_serialized = reader.read(tfrecord_file_queue)\n",
    "\n",
    "    # label and image are stored as bytes but could be stored as \n",
    "    # int64 or float64 values in a serialized tf.Example protobuf.\n",
    "    tfrecord_features = tf.parse_single_example(tfrecord_serialized,\n",
    "                        features={\n",
    "                            'seq/text': tf.VarLenFeature(tf.string),\n",
    "#                             'shape': tf.VarLenFeature(tf.string),\n",
    "                            'seq/labels': tf.VarLenFeature(tf.string),\n",
    "                        }, name='features')\n",
    "\n",
    "    text = tf.cast(tfrecord_features['seq/text'], tf.string)\n",
    "    labels = tf.cast(tfrecord_features['seq/labels'], tf.string)\n",
    "    return text, labels\n",
    "\n",
    "def read_tfrecord(tfrecord_file):\n",
    "    text, labels = read_from_tfrecord([tfrecord_file])\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        for _ in range(5):\n",
    "            text_retrieved, labels_retrieved = sess.run([text, labels])\n",
    "            print(text_retrieved.values[0])\n",
    "            print(labels_retrieved.values[0])\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Jean Pierre lives in New York .'\n",
      "b'B-PER I-PER O O B-LOC I-LOC O'\n",
      "b'The European Union is a political and economic union'\n",
      "b'O B-ORG I-ORG O O O O O O'\n",
      "b'A French American actor won an oscar'\n",
      "b'O B-MISC I-MISC O O O O'\n",
      "b'Jean Pierre lives in New York .'\n",
      "b'B-PER I-PER O O B-LOC I-LOC O'\n",
      "b'The European Union is a political and economic union'\n",
      "b'O B-ORG I-ORG O O O O O O'\n"
     ]
    }
   ],
   "source": [
    "read_tfrecord(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use it with Dataset API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.contrib.data.python.ops.dataset_ops.Iterator at 0x7f6bf4f23f60>"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = tf.placeholder(tf.string, shape=[None])\n",
    "\n",
    "# Build dataset iterator\n",
    "dataset = tf.contrib.data.TFRecordDataset(filenames)\n",
    "\n",
    "dataset = dataset.repeat(None)  # Infinite iterations\n",
    "# if shuffle:\n",
    "#     dataset = dataset.shuffle(buffer_size=10000)\n",
    "dataset = dataset.batch(12)\n",
    "iterator_tf = dataset.make_initializable_iterator()\n",
    "\n",
    "iterator_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ b'\\nb\\n/\\n\\nseq/labels\\x12!\\n\\x1f\\n\\x1dB-PER I-PER O O B-LOC I-LOC O\\n/\\n\\x08seq/text\\x12#\\n!\\n\\x1fJean Pierre lives in New York .'\n",
      " b'\\ns\\nD\\n\\x08seq/text\\x128\\n6\\n4The European Union is a political and economic union\\n+\\n\\nseq/labels\\x12\\x1d\\n\\x1b\\n\\x19O B-ORG I-ORG O O O O O O'\n",
      " b'\\na\\n)\\n\\nseq/labels\\x12\\x1b\\n\\x19\\n\\x17O B-MISC I-MISC O O O O\\n4\\n\\x08seq/text\\x12(\\n&\\n$A French American actor won an oscar'\n",
      " b'\\nb\\n/\\n\\x08seq/text\\x12#\\n!\\n\\x1fJean Pierre lives in New York .\\n/\\n\\nseq/labels\\x12!\\n\\x1f\\n\\x1dB-PER I-PER O O B-LOC I-LOC O'\n",
      " b'\\ns\\nD\\n\\x08seq/text\\x128\\n6\\n4The European Union is a political and economic union\\n+\\n\\nseq/labels\\x12\\x1d\\n\\x1b\\n\\x19O B-ORG I-ORG O O O O O O'\n",
      " b'\\na\\n)\\n\\nseq/labels\\x12\\x1b\\n\\x19\\n\\x17O B-MISC I-MISC O O O O\\n4\\n\\x08seq/text\\x12(\\n&\\n$A French American actor won an oscar'\n",
      " b'\\nb\\n/\\n\\x08seq/text\\x12#\\n!\\n\\x1fJean Pierre lives in New York .\\n/\\n\\nseq/labels\\x12!\\n\\x1f\\n\\x1dB-PER I-PER O O B-LOC I-LOC O'\n",
      " b'\\ns\\nD\\n\\x08seq/text\\x128\\n6\\n4The European Union is a political and economic union\\n+\\n\\nseq/labels\\x12\\x1d\\n\\x1b\\n\\x19O B-ORG I-ORG O O O O O O'\n",
      " b'\\na\\n)\\n\\nseq/labels\\x12\\x1b\\n\\x19\\n\\x17O B-MISC I-MISC O O O O\\n4\\n\\x08seq/text\\x12(\\n&\\n$A French American actor won an oscar'\n",
      " b'\\nb\\n/\\n\\x08seq/text\\x12#\\n!\\n\\x1fJean Pierre lives in New York .\\n/\\n\\nseq/labels\\x12!\\n\\x1f\\n\\x1dB-PER I-PER O O B-LOC I-LOC O'\n",
      " b'\\ns\\n+\\n\\nseq/labels\\x12\\x1d\\n\\x1b\\n\\x19O B-ORG I-ORG O O O O O O\\nD\\n\\x08seq/text\\x128\\n6\\n4The European Union is a political and economic union'\n",
      " b'\\nb\\n/\\n\\nseq/labels\\x12!\\n\\x1f\\n\\x1dB-PER I-PER O O B-LOC I-LOC O\\n/\\n\\x08seq/text\\x12#\\n!\\n\\x1fJean Pierre lives in New York .']\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "sess.run(iterator_tf.initializer,\n",
    "        feed_dict={filenames: [save_path]})\n",
    "res = sess.run(iterator_tf.get_next())\n",
    "print(res)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the inputs\n",
    "def setup_input_graph(tfrecord_filenames, batch_size, shuffle=True, scope='train-data'):\n",
    "    \"\"\"Return the input function to get the training data.\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): Batch size of training iterator that is returned\n",
    "                          by the input function.\n",
    "        mnist_data (Object): Object holding the loaded mnist data.\n",
    "\n",
    "    Returns:\n",
    "        (Input function, IteratorInitializerHook):\n",
    "            - Function that returns (features, labels) when called.\n",
    "            - Hook to initialise input iterator.\n",
    "    \"\"\"\n",
    "    iterator_initializer_hook = IteratorInitializerHook()\n",
    "\n",
    "\n",
    "    def inputs():\n",
    "        \"\"\"Returns training set as Operations.\n",
    "\n",
    "        Returns:\n",
    "            (features, labels) Operations that iterate over the dataset\n",
    "            on every evaluation\n",
    "        \"\"\"\n",
    "        with tf.name_scope(scaope):\n",
    "            \n",
    "            filenames = tf.placeholder(tf.string, shape=[None])\n",
    "            \n",
    "            # Build dataset iterator\n",
    "            dataset = tf.data.TFRecordDataset(filenames)\n",
    "            \n",
    "            dataset = dataset.repeat(None)  # Infinite iterations\n",
    "            if shuffle:\n",
    "                dataset = dataset.shuffle(buffer_size=10000)\n",
    "            dataset = dataset.batch(batch_size)\n",
    "            iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "            # Set runhook to initialize iterator\n",
    "            iterator_initializer_hook.iterator_initializer_func = \\\n",
    "                lambda sess: sess.run(\n",
    "                    iterator.initializer,\n",
    "                    feed_dict={filenames: tfrecord_filenames})\n",
    "\n",
    "            next_example, next_label = iterator.get_next()\n",
    "\n",
    "            # Return batched (features, labels)\n",
    "            return next_example, next_label\n",
    "\n",
    "    # Return function and hook\n",
    "    return inputs, iterator_initializer_hook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "- [https://github.com/visipedia/tfrecords/blob/master/create_tfrecords.py](https://github.com/visipedia/tfrecords/blob/master/create_tfrecords.py)\n",
    "- [https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html](https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
