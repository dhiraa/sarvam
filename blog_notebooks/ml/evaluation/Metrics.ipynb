{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "A confusion matrix is an N X N matrix, where N is the number of classes being predicted. For the problem in hand, we have N=2, and hence we get a 2 X 2 matrix. Here are a few definitions, you need to remember for a confusion matrix :\n",
    "\n",
    "- Accuracy : the proportion of the total number of predictions that were correct.\n",
    "- Positive Predictive Value or Precision : the proportion of positive cases that were correctly identified.\n",
    "- Negative Predictive Value : the proportion of negative cases that were correctly identified.\n",
    "- Sensitivity or Recall : the proportion of actual positive cases which are correctly identified.\n",
    "- Specificity : the proportion of actual negative cases which are correctly identified.\n",
    "\n",
    "![](Confusion_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area Under the ROC curve (AUC – ROC)\n",
    "\n",
    "This is again one of the popular metrics used in the industry.  The biggest advantage of using ROC curve is that it is independent of the change in proportion of responders. This statement will get clearer in the following sections.\n",
    "\n",
    "Let’s first try to understand what is ROC (Receiver operating characteristic) curve. If we look at the confusion matrix below, we observe that for a probabilistic model, we get different value for each metric.\n",
    "\n",
    "\n",
    "![](Confusion_matrix.png)\n",
    "\n",
    "Hence, for each sensitivity, we get a different specificity.The two vary as follows:\n",
    "\n",
    "![](curves.png)\n",
    "\n",
    "The ROC curve is the plot between sensitivity and (1- specificity). (1- specificity) is also known as false positive rate and sensitivity is also known as True Positive rate. Following is the ROC curve for the case in hand.\n",
    "\n",
    "![](ROC.png)\n",
    "\n",
    "Let’s take an example of threshold = 0.5 (refer to confusion matrix). Here is the confusion matrix :\n",
    "\n",
    "![](Confusion_matrix2.png)\n",
    "\n",
    "\n",
    "As you can see, the sensitivity at this threshold is 99.6% and the (1-specificity) is ~60%. This coordinate becomes on point in our ROC curve. To bring this curve down to a single number, we find the area under this curve (AUC).\n",
    "\n",
    "Note that the area of entire square is 1*1 = 1. Hence AUC itself is the ratio under the curve and the total area. For the case in hand, we get AUC ROC as 96.4%. Following are a few thumb rules:\n",
    "\n",
    "- .90-1 = excellent (A)\n",
    "- .80-.90 = good (B)\n",
    "- .70-.80 = fair (C)\n",
    "- .60-.70 = poor (D)\n",
    "- .50-.60 = fail (F)\n",
    "\n",
    "We see that we fall under the excellent band for the current model. But this might simply be over-fitting. In such cases it becomes very important to to in-time and out-of-time validations.\n",
    "\n",
    "Points to Remember:\n",
    "\n",
    "1. For a model which gives class as output, will be represented as a single point in ROC plot.\n",
    "\n",
    "2. Such models cannot be compared with each other as the judgement needs to be taken on a single metric and not using multiple metrics. For instance, model with parameters (0.2,0.8) and model with parameter (0.8,0.2) can be coming out of the same model, hence these metrics should not be directly compared.\n",
    "\n",
    "3. In case of probabilistic model, we were fortunate enough to get a single number which was AUC-ROC. But still, we need to look at the entire curve to make conclusive decisions. It is also possible that one model performs better in some region and other performs better in other.\n",
    "\n",
    "## Advantages of using ROC\n",
    "\n",
    "Why should you use ROC and not metrics like lift curve?\n",
    "\n",
    "Lift is dependent on total response rate of the population. Hence, if the response rate of the population changes, the same model will give a different lift chart. A solution to this concern can be true lift chart (finding the ratio of lift and perfect model lift at each decile). But such ratio rarely makes sense for the business.\n",
    "\n",
    "ROC curve on the other hand is almost independent of the response rate. This is because it has the two axis coming out from columnar calculations of confusion matrix. The numerator and denominator of both x and y axis will change on similar scale in case of response rate shift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "- https://www.analyticsvidhya.com/blog/2016/02/7-important-model-evaluation-error-metrics/\n",
    "- https://en.wikipedia.org/wiki/Precision_and_recall  \n",
    "- https://en.wikipedia.org/wiki/F1_score  \n",
    "- https://en.wikipedia.org/wiki/Receiver_operating_characteristic  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
