{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title:  \"tf.nn.*rnn* APIs\"\n",
    "description: \"Some Intersting Facts About RNN APIs\"\n",
    "excerpt: \"Some Intersting Facts About RNN APIs\"\n",
    "date:   2017-11-11\n",
    "mathjax: true\n",
    "comments: true \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jupyter notebook avaialble @ [https://github.com/dhiraa/tf-guru/blob/master/rnn/2017-11-11-tf.nn.rnn-APIs.ipynb](https://github.com/dhiraa/tf-guru/blob/master/rnn/2017-11-11-tf.nn.rnn-APIs.ipynb)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# static_rnn vs. dynamic_rnn\n",
    "\n",
    "You may have noticed that Tensorflow contains two different functions for RNNs: [tf.nn.static_rnn](https://www.tensorflow.org/api_docs/python/tf/nn/static_rnn) and [tf.nn.dynamic_rnn](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn). Which one to use?\n",
    "\n",
    "\n",
    "Internally, tf.nn.rnn creates an unrolled graph for a fixed RNN length. That means, if you call tf.nn.rnn with inputs having 200 time steps you are creating a static graph with 200 RNN steps. First, graph creation is slow. Second, you’re unable to pass in longer sequences (> 200) than you’ve originally specified.\n",
    "\n",
    "`tf.nn.dynamic_rnn` solves this. It uses a `tf.While` loop to dynamically construct the graph when it is executed. That means graph creation is faster and you can feed batches of variable size. What about performance? You may think the static rnn is faster than its dynamic counterpart because it pre-builds the graph. In my experience that’s not the case.\n",
    "\n",
    "In short, just use `tf.nn.dynamic_rnn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "SEQUENCE_LENGTH = 10\n",
    "NUMBER_OF_WORDS = 8\n",
    "# Create input data\n",
    "X = np.random.randn(BATCH_SIZE, SEQUENCE_LENGTH, NUMBER_OF_WORDS)\n",
    "\n",
    "# The second example is of length 6 \n",
    "X[1,6:] = 0\n",
    "X_lengths = [10, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(\":/cpu:0\"):\n",
    "    data = tf.placeholder(tf.float64, [None,  SEQUENCE_LENGTH, NUMBER_OF_WORDS])\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(num_units=8, state_is_tuple=True)\n",
    "\n",
    "    # defining initial state\n",
    "    initial_state = cell.zero_state(BATCH_SIZE, dtype=tf.float64)\n",
    "\n",
    "    encode = tf.nn.dynamic_rnn(\n",
    "        cell=cell,\n",
    "        dtype=tf.float64,\n",
    "        sequence_length=X_lengths,\n",
    "        inputs=data,\n",
    "        initial_state=initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "outputs, last_states = sess.run(encode, feed_dict={data: X})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.0283708 , -0.02738941,  0.0755726 , -0.08008824,  0.01081016,\n",
       "         -0.03213166, -0.0536146 , -0.15033308],\n",
       "        [-0.07012742,  0.07191048,  0.16121555, -0.06183664,  0.10730578,\n",
       "          0.05112168,  0.20581951, -0.03912028],\n",
       "        [ 0.1738402 ,  0.26620728,  0.10301632,  0.13077066,  0.0349601 ,\n",
       "          0.0586684 ,  0.27258061,  0.32184957],\n",
       "        [-0.00453193,  0.05837649,  0.00355916, -0.08189783,  0.01207369,\n",
       "          0.08217721,  0.09261072,  0.1040738 ],\n",
       "        [ 0.07379494,  0.06887853,  0.02938879, -0.04013838, -0.01955719,\n",
       "          0.02584432,  0.09734795,  0.12879024],\n",
       "        [ 0.20445797,  0.04512449, -0.03814287,  0.10622528,  0.02263981,\n",
       "         -0.17649335,  0.24971871,  0.36151143],\n",
       "        [ 0.20608251,  0.13480439,  0.10275332,  0.04439773, -0.00303557,\n",
       "         -0.27451886,  0.17191344,  0.31303354],\n",
       "        [ 0.32365047,  0.48470812,  0.17407803,  0.2052482 , -0.02023543,\n",
       "         -0.17407063,  0.29517553,  0.48837349],\n",
       "        [ 0.1345457 ,  0.33321533,  0.28944614,  0.09551912,  0.03292939,\n",
       "         -0.20709579,  0.05701367,  0.12379222],\n",
       "        [ 0.17016332,  0.06186617,  0.15829436,  0.16037617, -0.15841691,\n",
       "         -0.3717137 ,  0.11021478,  0.22602701]],\n",
       "\n",
       "       [[ 0.04793363,  0.12344369, -0.1783383 ,  0.04842179,  0.15124708,\n",
       "          0.05783913,  0.09932703, -0.04334414],\n",
       "        [-0.08500263,  0.29018905, -0.13871866,  0.14451168, -0.01455011,\n",
       "          0.03964509,  0.2207585 ,  0.1708107 ],\n",
       "        [-0.07214073,  0.21678313, -0.11355709,  0.04147981,  0.11652134,\n",
       "          0.03886285,  0.04392174,  0.20238952],\n",
       "        [-0.11234446,  0.16728768, -0.14256807, -0.09604893, -0.05138749,\n",
       "          0.1120749 ,  0.12973008,  0.26944325],\n",
       "        [-0.08622686,  0.20366575, -0.0149762 , -0.14269766, -0.32314589,\n",
       "          0.02864775,  0.06804786,  0.24432493],\n",
       "        [-0.05230012,  0.42949286, -0.07228138, -0.04229116, -0.11640012,\n",
       "         -0.06838009,  0.30524024,  0.28048882],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=array([[ 0.26831959,  0.33156717,  0.33671164,  0.2987655 , -0.24417013,\n",
       "        -0.92449115,  0.30798403,  0.61355526],\n",
       "       [-0.14194088,  0.71295599, -0.10051528, -0.11628008, -0.26955969,\n",
       "        -0.15300985,  0.45661472,  0.54125506]]), h=array([[ 0.17016332,  0.06186617,  0.15829436,  0.16037617, -0.15841691,\n",
       "        -0.3717137 ,  0.11021478,  0.22602701],\n",
       "       [-0.05230012,  0.42949286, -0.07228138, -0.04229116, -0.11640012,\n",
       "        -0.06838009,  0.30524024,  0.28048882]]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Bidirectional RNNs\n",
    "\n",
    "When using a standard RNN to make predictions we are only taking the “past” into account. For certain tasks this makes sense (e.g. predicting the next word), but for some tasks it would be useful to take both the past and the future into account. Think of a tagging task, like part-of-speech tagging, where we want to assign a tag to each word in a sentence. Here we already know the full sequence of words, and for each word we want to take not only the words to the left (past) but also the words to the right (future) into account when making a prediction. Bidirectional RNNs do exactly that. A bidirectional RNN is a combination of two RNNs – one runs forward from “left to right” and one runs backward from “right to left”. These are commonly used for tagging tasks, or when we want to embed a sequence into a fixed-length vector (beyond the scope of this post).\n",
    "\n",
    "Just like for standard RNNs, Tensorflow has [static](https://www.tensorflow.org/api_docs/python/tf/nn/static_bidirectional_rnn) and [dynamic](https://www.tensorflow.org/api_docs/python/tf/nn/bidirectional_dynamic_rnn) versions of the bidirectional RNN. The bidirectional_dynamic_rnn is preferred over the static bidirectional_rnn.\n",
    "\n",
    "The key differences of the bidirectional RNN functions are that they take a separate cell argument for both the forward and backward RNN, and that they return separate outputs and states for both the forward and backward RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(\":/cpu:0\"):\n",
    "    data = tf.placeholder(tf.float64, [None,  SEQUENCE_LENGTH, NUMBER_OF_WORDS])\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(num_units=4, state_is_tuple=True)\n",
    "\n",
    "    # defining initial state\n",
    "    initial_state = cell.zero_state(BATCH_SIZE, dtype=tf.float64)\n",
    "\n",
    "    encode = tf.nn.dynamic_rnn(\n",
    "        cell=cell,\n",
    "        dtype=tf.float64,\n",
    "        sequence_length=X_lengths,\n",
    "        inputs=data,\n",
    "        initial_state=initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "outputs, states = sess.run(encode, feed_dict={data: X})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_fw, output_bw = outputs\n",
    "states_fw, states_bw = states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SEQUENCE_LENGTH, LSTM_HIDDEN_SIZE\n",
    "output_bw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##BATCH_SIZE, LSTM_HIDDEN_SIZE\n",
    "states_bw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook 2017-11-11-tf.nn.rnn-APIs.ipynb to markdown\n",
      "[NbConvertApp] Writing 8155 bytes to ../docs/_posts/2017-11-11-tf.nn.rnn-APIs.md\n"
     ]
    }
   ],
   "source": [
    "# Convert this notebook for Docs\n",
    "! jupyter nbconvert --to markdown --output-dir ../docs/_posts 2017-11-11-tf.nn.rnn-APIs.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
